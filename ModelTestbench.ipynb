{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "937a9bf7f4d02271b0e84c824f2cfec9d499b9b3c4d4ea54ebae47f399b1cd48"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Model Tests\n",
    "This Notebook is to train or test specific models."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "seed_val = 2000\n",
    "np.random.seed(seed_val)\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset, seed, train_ratio=0.6, shuffle=True, highest_at_top=False):\n",
    "    if shuffle:\n",
    "        dataset = dataset.sample(frac=1, random_state=seed)#.reset_index(drop=True)\n",
    "    \n",
    "    if highest_at_top:\n",
    "        df = dataset.copy()\n",
    "        sub_115 = df.drop( df[ df[\"eta c\"]>0.9 ].index)\n",
    "        big_115 = df[df[\"eta c\"] > 0.9]\n",
    "        # shuffle eta c < 1.15 data\n",
    "        sub_115 = sub_115.sample(frac=1, random_state=seed)\n",
    "        # join the data\n",
    "        x = big_115.append(sub_115)\n",
    "\n",
    "        train_dataset = x[0:int(len(x)*train_ratio)]\n",
    "        test_dataset = dataset.drop(train_dataset.index)\n",
    "        return x, test_dataset\n",
    "    \n",
    "    train_dataset = dataset.sample(frac=train_ratio, random_state=0)\n",
    "    test_dataset = dataset.drop(train_dataset.index)\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def add_bias(data):\n",
    "    N1 = np.shape(data)[0]\n",
    "    N2 = np.shape(data)[1]\n",
    "    a = -1*np.ones((N1,N2+1))\n",
    "    a[:,:-1] = data\n",
    "    return a\n",
    "\n",
    "def add_noise(dataset, target_column=4, noise_var=0.01, input_n=False, output_n=False):\n",
    "    \"\"\" Called on DATAFRAME training data. \"\"\"\n",
    "    features = dataset.to_numpy()[:,0:target_column]\n",
    "    labels = np.reshape(dataset.to_numpy()[:,target_column], (-1,1))\n",
    "    \n",
    "    if input_n:\n",
    "        noise = np.reshape(np.random.normal(0,0.01,np.shape(features)[0]*np.shape(features)[1]),(np.shape(features)[0],np.shape(features)[1]))\n",
    "        features = features + noise \n",
    "\n",
    "    if output_n:\n",
    "        noise = np.reshape(np.random.normal(0,0.01,np.shape(labels)[0]*np.shape(labels)[1]),(np.shape(labels)[0],np.shape(labels)[1]))\n",
    "        labels = labels + noise\n",
    "    return features, labels\n",
    "\n",
    "def check_eta_range(dataset, nums=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3,1.4]):\n",
    "    # nums = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3,1.4]\n",
    "    less_than_percents = []\n",
    "    in_range_percents = []\n",
    "    x_ranges = []\n",
    "\n",
    "    # lim_dataset[ (lim_dataset[\"b1\"]<0.1) & (lim_dataset[\"RatioTotalArea\"]<0.2) ]\n",
    "\n",
    "    for i in range(len(nums)):\n",
    "        less_than_percents.append( ((len(dataset[dataset[\"eta c\"] <= nums[i]]))/len(dataset))*100)\n",
    "        in_range_percents.append( (len(dataset[ (dataset[\"eta c\"]<nums[i]) & (dataset[\"eta c\"] >= nums[i-1]) ])/len(dataset))*100 )\n",
    "        x_ranges.append(\"{one}<EtaC<{two}\".format(one=nums[i-1], two=nums[i]) )\n",
    "\n",
    "    nums.pop(0)\n",
    "    less_than_percents.pop(0)\n",
    "    in_range_percents.pop(0)\n",
    "    x_ranges.pop(0)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(15,8))\n",
    "\n",
    "    ax[0].scatter(nums, less_than_percents)\n",
    "    ax[0].set_ylabel(\"Percentage\")\n",
    "    ax[0].set_xlabel(\"Eta c\")\n",
    "    ax[0].set_title(\"Percentage of data with Eta C less than x\")\n",
    "    for i in range(len(less_than_percents)):\n",
    "        if less_than_percents[i] > 99.0:\n",
    "            ax[0].scatter(nums[i], less_than_percents[i], c=\"red\", label=\">99%\")\n",
    "    ax[0].legend()\n",
    "\n",
    "\n",
    "\n",
    "    ax[1].scatter(x_ranges, in_range_percents)\n",
    "    ax[1].set_ylabel(\"Percentage\")\n",
    "    ax[1].set_xlabel(\"Eta c\")\n",
    "    ax[1].set_title(\"Percentage of data with Eta C within range\")\n",
    "    for i in range(len(in_range_percents)):\n",
    "        if sum(in_range_percents[0:i])>99:\n",
    "            ax[1].scatter(x_ranges[i], in_range_percents[i], c=\"red\", label=\"sum>99%\")\n",
    "    ax[1].legend()\n",
    "\n",
    "    plt.xticks(rotation='vertical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"LIM_scaled.csv\"\n",
    "name = \"data.csv\"\n",
    "\n",
    "dataset = pd.read_csv(name)\n",
    "# data2.describe().transpose()\n",
    "dataset.pop(\"Unnamed: 0\")\n",
    "\n",
    "# scale the data\n",
    "scaled_dataset = dataset.copy()\n",
    "scaled_dataset['b1'] = MaxAbsScaler().fit_transform(dataset['b1'].values.reshape(-1,1))\n",
    "scaled_dataset['a2'] = MaxAbsScaler().fit_transform(dataset['a2'].values.reshape(-1,1))\n",
    "scaled_dataset['RatioTotalArea'] = MaxAbsScaler().fit_transform(dataset['RatioTotalArea'].values.reshape(-1,1))\n",
    "scaled_dataset['frac'] = MaxAbsScaler().fit_transform(dataset['frac'].values.reshape(-1,1))\n",
    "\n",
    "# split the data\n",
    "train_dataset, test_dataset = split_data(scaled_dataset.copy(), seed_val, train_ratio=0.8, highest_at_top=True)\n",
    "\n",
    "# sort the data to check representation\n",
    "# sorted_train = train_dataset.sort_index()\n",
    "sorted_train = train_dataset\n",
    "sorted_test = test_dataset.sort_index()\n",
    "\n",
    "# check representation\n",
    "fig = plt.figure()\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(15,7))\n",
    "\n",
    "ax[0].scatter(x=np.arange(len(sorted_train)), y=sorted_train['eta c'], marker='.', alpha=0.4)\n",
    "ax[1].scatter(x=np.arange(len(sorted_test)), y=sorted_test['eta c'], marker='.', alpha=0.4)\n",
    "ax[0].set_ylabel(\"eta c\")\n",
    "ax[0].set_title(\"Training Data\")\n",
    "ax[1].set_title(\"Testing Data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_eta_range(train_dataset.copy(), nums=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3,1.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_eta_range(test_dataset.copy(), nums=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3,1.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Regression():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def learning_rate(self, initial_lr=1e-2, decay_steps=1e5, decay_rate=0.9):\n",
    "        self.lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=initial_lr, decay_steps=decay_steps, decay_rate=decay_rate)\n",
    "\n",
    "    def SGDoptimizer(self, momentum=0.1, nesterov=False, initial_lr=1e-2, decay_steps=1e5, decay_rate=0.9):\n",
    "        self.learning_rate(initial_lr, decay_steps, decay_rate)\n",
    "        self.optimizer = keras.optimizers.SGD(learning_rate=self.lr_schedule, momentum=momentum, nesterov=nesterov)\n",
    "\n",
    "    def ADAMoptimizer(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False):\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, amsgrad=amsgrad, name=\"Adam\")\n",
    "\n",
    "    def add_layer(self, num_nodes, kernel_reg=None):\n",
    "        self.model.add(Dense(num_nodes, activation=\"relu\", use_bias=True, kernel_regularizer=kernel_reg))\n",
    "    \n",
    "    def add_output_layer(self, out_nodes, kernel_reg=None):\n",
    "        self.model.add(Dense(out_nodes, activation=\"relu\", use_bias=True, kernel_regularizer=kernel_reg))\n",
    "\n",
    "    def add_dropout(self, rate, seed):\n",
    "        self.model.add(keras.layers.Dropout(rate=rate, seed=seed))\n",
    "\n",
    "    def compileModel(self, optimizer, loss=\"mean_squared_error\", metrics=[\"MSE\",\"MAE\", \"MAPE\"]):\n",
    "        self.model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "    def train_model(self, X, Y, early_stopping=True, patience=3, epochs=10, batch_size=500, vali_split=0.2, shuffle=False):\n",
    "        if early_stopping:\n",
    "            earlystop_callback = EarlyStopping(monitor=\"loss\", min_delta=0, patience=patience, mode=\"min\", restore_best_weights=True)\n",
    "            hist = self.model.fit(X, Y, epochs=epochs, batch_size=batch_size, validation_split=vali_split, callbacks=[earlystop_callback], shuffle=shuffle)\n",
    "            self.history = pd.DataFrame(hist.history)\n",
    "            self.history['epoch'] = hist.epoch\n",
    "        else:\n",
    "            hist = self.model.fit(X, Y, epochs=epochs, batch_size=batch_size, validation_split=vali_split, shuffle=shuffle)\n",
    "            self.history = pd.DataFrame(hist.history)\n",
    "            self.history['epoch'] = hist.epoch\n",
    "\n",
    "    def show_training_errors(self):\n",
    "        hist = self.history\n",
    "\n",
    "        fig1 = plt.figure()\n",
    "        fig1, ax = plt.subplots(nrows=2, ncols=2, figsize=(15,12))\n",
    "\n",
    "        ax[0][0].set_xlabel('Epoch')\n",
    "        ax[0][0].set_ylabel('MSE')\n",
    "        ax[0][0].plot(hist['epoch'], hist['MSE'], label='Train Error')\n",
    "        ax[0][0].plot(hist['epoch'], hist['val_MSE'], label='Val Error')\n",
    "        ax[0][0].legend()\n",
    "        ax[0][0].set_title(\"MSE Error\")\n",
    "\n",
    "        ax[0][1].set_xlabel('Epoch')\n",
    "        ax[0][1].set_ylabel('MAE')\n",
    "        ax[0][1].plot(hist['epoch'], hist['MAE'], label='Train Error')\n",
    "        ax[0][1].plot(hist['epoch'], hist['val_MAE'], label='Val Error')\n",
    "        ax[0][1].legend()\n",
    "        ax[0][1].set_title(\"MAE Error\")\n",
    "\n",
    "        ax[1][0].set_xlabel('Epoch')\n",
    "        ax[1][0].set_ylabel('MAPE')\n",
    "        ax[1][0].plot(hist['epoch'], hist['MAPE'], label='Mean Abs {} Error'.format(\"%\"))\n",
    "        ax[1][0].legend()\n",
    "        ax[1][0].set_title(\"MAPE\")\n",
    "\n",
    "    def test(self, test_features, test_labels):\n",
    "        test_input = test_features\n",
    "        test_output = np.reshape(test_labels, (-1,1))\n",
    "        prediction = self.model.predict(test_input)\n",
    "        error = abs(test_output-prediction)\n",
    "        acc = (error/test_output)*100\n",
    "        \n",
    "\n",
    "        fig2 = plt.figure()\n",
    "        fig2, ax = plt.subplots(nrows=3, figsize=(15,16))\n",
    "        ax[0].plot(np.arange(len(test_output)), test_output, label=\"TestData\")\n",
    "        ax[0].plot(np.arange(len(prediction)), prediction, label=\"Prediction\", alpha=0.4)\n",
    "        ax[0].legend()\n",
    "        ax[0].set_title(\"Model Prediction\")\n",
    "        ax[0].set_ylabel(\"Eta c\")\n",
    "\n",
    "        ax[1].plot(np.arange(len(error)), error, label=\"abs(test_label-pred)\")\n",
    "        ax[1].legend()\n",
    "        ax[1].set_ylabel(\"Error\")\n",
    "        # ax[1].set_title(\"Model Prediction\")\n",
    "\n",
    "        ax[2].plot(np.arange(len(acc)), acc, label=\"(error/test_label)*100\")\n",
    "        ax[2].legend()\n",
    "        ax[2].set_ylabel(\"MAPE\")\n",
    "        # ax[1].set_title(\"Model Prediction\")\n",
    "    \n",
    "    def save(self, loc_and_name):\n",
    "        self.model.save(\"/media/nirav/34E0-F309/KTH/Thesis/ModelSummaries/Implementable/{}\".format(loc_and_name))\n",
    "        self.history.to_csv(\"/media/nirav/34E0-F309/KTH/Thesis/ModelSummaries/Implementable/{}.csv\".format(loc_and_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loaded_model = keras.models.load_model(\"/media/nirav/34E0-F309/KTH/Thesis/ModelSummaries/Implementable/33_26_ADAM23\")\n",
    "\n",
    "# Feature Selections\n",
    "# for noisey TRAINING data use this instead \n",
    "# train_features, train_labels = add_noise(train_dataset, noise_var=0.0001, output_n=True)\n",
    "# split the data\n",
    "train_features = train_dataset.to_numpy()[:,0:4]\n",
    "train_labels = train_dataset.to_numpy()[:,4]\n",
    "\n",
    "# test data\n",
    "sorted_test = test_dataset.sort_index()\n",
    "test_features = sorted_test.to_numpy()[:,0:4]\n",
    "test_labels = sorted_test.to_numpy()[:,4]\n",
    "\n",
    "X = train_features.copy()\n",
    "Y = train_labels.copy()\n",
    "\n",
    "#_________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
    "# setup algo\n",
    "my_model = MLP_Regression(loaded_model)\n",
    "my_model.ADAMoptimizer(learning_rate=0.000001, beta_1=0.95)\n",
    "# my_model.SGDoptimizer(momentum=0.6, nesterov=True, initial_lr=1e-1, decay_steps=1e5, decay_rate=0.9)\n",
    "\n",
    "# run algo\n",
    "my_model.compileModel(my_model.optimizer, loss=\"mean_absolute_percentage_error\")\n",
    "my_model.train_model(X,Y, epochs=1, patience=10, batch_size=32, shuffle=True, early_stopping=True, vali_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "# my_model.save(\"33_26_ADAM23\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.show_training_errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_features = sorted_test.sort_values(\"eta c\").to_numpy()[:,0:4]\n",
    "# test_labels = sorted_test.sort_values(\"eta c\").to_numpy()[:,4]\n",
    "\n",
    "test_features = sorted_train.sort_values(\"eta c\").to_numpy()[:,0:4]\n",
    "test_labels = sorted_train.sort_values(\"eta c\").to_numpy()[:,4]\n",
    "\n",
    "my_model.test(test_features, test_labels)"
   ]
  },
  {
   "source": [
    "## Test without training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = keras.models.load_model(\"/media/nirav/34E0-F309/KTH/Thesis/ModelSummaries/Implementable/33_26_ADAM19\")\n",
    "\n",
    "# Feature Selections\n",
    "# for noisey TRAINING data use this instead \n",
    "# train_features, train_labels = add_noise(train_dataset, noise_var=0.0001, output_n=True)\n",
    "# split the data\n",
    "train_features = train_dataset.to_numpy()[:,0:4]\n",
    "train_labels = train_dataset.to_numpy()[:,4]\n",
    "\n",
    "# test data\n",
    "sorted_test = test_dataset.sort_index()\n",
    "test_features = sorted_test.to_numpy()[:,0:4]\n",
    "test_labels = sorted_test.to_numpy()[:,4]\n",
    "\n",
    "X = train_features.copy()\n",
    "Y = train_labels.copy()\n",
    "\n",
    "#_________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
    "# setup algo\n",
    "my_model = MLP_Regression(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_train_dataset = train_dataset.sort_index()\n",
    "train_features = sorted_train_dataset.to_numpy()[:,0:4]\n",
    "train_labels = sorted_train_dataset.to_numpy()[:,4]\n",
    "\n",
    "my_model.test(train_features, train_labels)"
   ]
  },
  {
   "source": [
    "We can then retrain the data. For instance, ADAM18 can be trained on a noisey set, then we can train on a non- noisey set. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}