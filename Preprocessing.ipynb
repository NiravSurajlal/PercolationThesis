{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "937a9bf7f4d02271b0e84c824f2cfec9d499b9b3c4d4ea54ebae47f399b1cd48"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Preprocessing\n",
    "This notebook attempts to make more sense of the data. The outcome from this notebook will be used to choose machine learning models and data augmentation methods. It will also help understanding the problem.\n",
    "\n",
    "## Assumptions\n",
    "- The long ellipse if given by r1 while the short ellips is given by a2 and r2\n",
    "- 2a1 = 1, thus a1=0.5 and b1=0.5. 2a2<=1.   \n",
    "- The hetrogenous ellipses are split with f stating how many are short. 1-f gives how many are long.\n",
    "- The ellipse ratio r, is given by the major and minor axis lengths such that r=a/b.\n",
    "- The width of the ellipse in a given axis direction is given by 2a or 2b."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# df[(df['A']>0) & (df['B']>0) & (df['C']>0)]"
   ]
  },
  {
   "source": [
    "Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(name='HybridEllipsePercolation.txt', sep1=\" \", header1=None, shuffle=True):\n",
    "    data = pd.read_csv(name, sep=sep1, header=header1)\n",
    "    data.columns = [\"r1\", \"2a2\", \"r2\", \"frac\", \"Nc\", \"Nc Std. Dev\", \"eta c\" ]\n",
    "    # data.reset_index(inplace=True)\n",
    "    \n",
    "    if shuffle:\n",
    "        data = data.sample(frac=1).reset_index(drop=True)\n",
    "    return data\n",
    "\n",
    "def single_input_vs_output(dataset, input_column, output_column=\"eta c\", plot=False, output=False):\n",
    "    reduced_dataset = dataset.drop_duplicates(subset=input_column)\n",
    "    # data_range = reduced_dataset[input_column].to_numpy()\n",
    "    if output:\n",
    "        print(\"{col} range is: {rng}    (output is {out})\".format(col=input_column, rng=len(reduced_dataset), out=output_column))\n",
    "\n",
    "    if plot:\n",
    "        ax1 = reduced_dataset.plot.scatter( x=input_column,\n",
    "                        y=output_column,\n",
    "                        c='DarkBlue')\n",
    "        return ax1\n",
    "        # return sns.scatterplot(data=reduced_dataset, x=input_column, y=output_column)\n",
    "    \n",
    "    return reduced_dataset[[input_column, output_column]]\n",
    "\n",
    "def split_data(dataset):\n",
    "    train_dataset = dataset.sample(frac=0.6, random_state=0)\n",
    "    valid_and_test_dataset = dataset.drop(train_dataset.index)\n",
    "    test_dataset = valid_and_test_dataset.sample(frac=0.5, random_state=0)\n",
    "    validation_dataset = valid_and_test_dataset.drop(test_dataset.index)\n",
    "    return train_dataset, test_dataset, validation_dataset\n",
    "\n",
    "def split_features_labels(data, label_column='eta c'):\n",
    "    features = data\n",
    "    labels = data.pop(label_column)\n",
    "    return features, labels"
   ]
  },
  {
   "source": [
    "Importing the data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = import_data(shuffle=False)\n",
    "dataset = rawdata.copy()\n",
    "\n",
    "# remove irrelevant columns\n",
    "# dataset.pop(\"Nc Std. Dev\")\n",
    "\n",
    "# check for missing values\n",
    "dataset.isna().sum()\n",
    "# drop missing values\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rawdata[\"frac\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe().transpose()"
   ]
  },
  {
   "source": [
    "From the statistics above, we can instantly see that the data is skewed. r1's mean is < 0.5*its ma, and the same goes for 2a2 and r2. we need to look at some distributions of the data. Maybe see some histrograms of frequency as well. If the data is skewed, splitting may not be simple. Some data may have to be left out to prevent biases. It is possible that we can consider this later during tuning. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Here we will extend the dataset to include the major and minor axis lengths and areas."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"a1\"]=1/2\n",
    "dataset[\"b1\"]=dataset[\"a1\"]/dataset[\"r1\"]\n",
    "dataset[\"a2\"]=dataset[\"2a2\"]/2\n",
    "dataset[\"b2\"]=dataset[\"a2\"]/dataset[\"r2\"]\n",
    "dataset[\"area1\"]=dataset[\"a1\"]*dataset[\"b1\"]*np.pi\n",
    "dataset[\"area2\"]=dataset[\"a2\"]*dataset[\"b2\"]*np.pi\n",
    "dataset[\"TotalArea\"] = dataset[\"area1\"] + dataset[\"area2\"]\n",
    "dataset[\"RatioTotalArea\"] = dataset[\"area1\"]*(1-dataset[\"frac\"]) + dataset[\"area2\"]*(dataset[\"frac\"])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe().transpose()"
   ]
  },
  {
   "source": [
    "The next step is to check for repeating values. Printing the returned variable gives all of the values. This (and some subsequent steps) have no consequence to the area, so the area will be left out. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1_range = single_input_vs_output(dataset, \"r1\")\n",
    "r2_range = single_input_vs_output(dataset, \"r2\")\n",
    "frac_range = single_input_vs_output(dataset, \"frac\")\n",
    "a1_range = single_input_vs_output(dataset, \"a1\")\n",
    "b1_range = single_input_vs_output(dataset, \"b1\")\n",
    "a2_range = single_input_vs_output(dataset, \"a2\")\n",
    "b2_range = single_input_vs_output(dataset, \"b2\")\n",
    "_2a2_range = single_input_vs_output(dataset, \"2a2\")\n",
    "\n",
    "dataset.nunique()"
   ]
  },
  {
   "source": [
    "You can see from theabove that a1 is constant. b2 has the most variation with frac varying the least. Next would be to plot a histogram of the numbers occuring for each one. The histogram plots will be presented as {r1, r2}, {a2,frac}. \n",
    "\n",
    "We have to note the actual numbers for choosing the correct bin size, so viewing the list of present values is useful.\n",
    "\n",
    "r1 -> 1000, r2 -> 1000, frac -> 100, a2 or 2a2 -> 1000"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.distplot(dataset[\"frac\"], kde=False, color='red', bins=100)\n",
    "# plt.title('Frequency of Fraction', fontsize=14)\n",
    "# plt.xlabel('Fraction', fontsize=10)\n",
    "# plt.ylabel('Frequency', fontsize=10)\n",
    "\n",
    "# fig, axs = plt.subplots(ncols=2, figsize=(12,6))\n",
    "# sns.distplot(dataset[\"r1\"], kde=False, color='red', bins=1000, ax=axs[0])\n",
    "# sns.distplot(dataset[\"r2\"], kde=False, color='red', bins=1000, ax=axs[1])\n",
    "# fig.savefig(\"imgs/r1_r2_freq.png\")"
   ]
  },
  {
   "source": [
    "r1 & r2 frequnecies.\n",
    "\n",
    "![r1_r2_freq](imgs/r1_r2_freq.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This is not too visible, so can convert the columns to strings and plot like catagories. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catagorical_dataset = dataset.copy()\n",
    "# catagorical_dataset['r1'] = catagorical_dataset['r1'].astype(str)\n",
    "# catagorical_dataset['r2'] = catagorical_dataset['r2'].astype(str)\n",
    "# fig1 = sns.displot(catagorical_dataset, x=\"r1\", shrink=.8, color='red', ax=axs[0])\n",
    "# fig2 = sns.displot(catagorical_dataset, x=\"r2\", shrink=.8, color='red', ax=axs[1])\n",
    "# fig1.savefig(\"imgs/cat_r1_freq.png\")\n",
    "# fig2.savefig(\"imgs/cat_r2_freq.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catagorical_dataset['frac'] = catagorical_dataset['frac'].astype(str)\n",
    "# fig1 = sns.displot(catagorical_dataset, x=\"frac\", shrink=.8, color='red', ax=axs[0])\n",
    "\n",
    "# catagorical_dataset['a2'] = catagorical_dataset['a2'].astype(str)\n",
    "# fig2 = sns.displot(catagorical_dataset, x=\"a2\", shrink=.8, color='red', ax=axs[1])\n",
    "# fig1.savefig(\"imgs/cat_frac_freq.png\")\n",
    "# fig2.savefig(\"imgs/cat_a2_freq.png\")"
   ]
  },
  {
   "source": [
    "From the plots, we can see that r1, r2 are relatively consistent in the amount of data for each r value, but the data is skewed towards the smaller r values.\n",
    "\n",
    "![cat_r1_freq](imgs/cat_r1_freq.png)\n",
    "![cat_r2_freq](imgs/cat_r2_freq.png)\n",
    "![cat_frac_freq](imgs/cat_frac_freq.png)\n",
    "![cat_a2_freq](imgs/cat_a2_freq.png)\n",
    "\n",
    "\n",
    "frac 0.99 has substantially fewer points than the rest. The frac class is also skewed towards the higher frac numbers, as the intervals decrease after 0.9 (including 0.95 and 0.99).\n",
    "\n",
    "the a2 class (or 2a2) increases by 0.05 between 0.05 and 0.5 (0.1 for 2a2...). However, it also includes {0.005, 0.01, 0.025}, i.e. +0.005, +0.015, +0.025. This means there are more points in the {0-0.05 range than the other intervals}. \n",
    "\n",
    "This can be shown in the distributino plots."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_range.sort_values('frac', inplace=True)\n",
    "_2a2_range.sort_values('2a2', inplace=True)\n",
    "a2_range.sort_values('a2', inplace=True)\n",
    "print(\"r1 range {}\".format(list(r1_range[\"r1\"])))\n",
    "print(\"r2 range {}\".format(list(r2_range[\"r2\"])))\n",
    "print(\"frac range {}\".format(list(frac_range[\"frac\"])))\n",
    "print(\"2a2 range {}\".format(list(_2a2_range[\"2a2\"])))\n",
    "print(\"a2 range {}\".format(list(a2_range[\"a2\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"r1 <= 100 is {x} out of {y}\".format( x=dataset[dataset[\"r1\"]<150].count()[\"r1\"], y=len(dataset)))\n",
    "print(\"r1=200 is {x}. r1=500 is {y}. r1=1000 is {z}\".format( x=dataset[dataset[\"r1\"]==200].count()[\"r1\"], y=dataset[dataset[\"r1\"]==500].count()[\"r1\"], z=dataset[dataset[\"r1\"]==1000].count()[\"r1\"]))\n",
    "print(\"r2 <= 100 is {x} out of {y}\".format( x=dataset[dataset[\"r2\"]<150].count()[\"r2\"], y=len(dataset)))\n",
    "print(\"r2=200 is {x}. r2=500 is {y}. r2=1000 is {z}\".format( x=dataset[dataset[\"r2\"]==200].count()[\"r2\"], y=dataset[dataset[\"r2\"]==500].count()[\"r2\"], z=dataset[dataset[\"r2\"]==1000].count()[\"r2\"]))"
   ]
  },
  {
   "source": [
    "For the r1 and r2 <=100 there is a lot of data. The other intervals 200, 500 and 1000 have a similar number of samples.Thi swould inidcate that we should remove from the lower set. Some options exist. If the lower space is equally involved, we could take the entire set <100, shuffle it and split it to an equal number of samples. Another option is to identify if a subset of this is more relevant to the predicitions/use cases. For instance, if most of the time we are in the <100 range, maybe the higher r values are unnecessary. The distribution plots make this clearer. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(ncols=2, figsize=(12,6))\n",
    "# sns.distplot(dataset[\"r1\"], color='red', bins=1000, ax=axs[0])\n",
    "# sns.distplot(dataset[\"r2\"], color='red', bins=1000, ax=axs[1])\n",
    "# fig.savefig(\"imgs/dist_r1_r2_freq.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(ncols=2, figsize=(12,6))\n",
    "# sns.distplot(dataset[\"frac\"], color='red', bins=100, ax=axs[0])\n",
    "# sns.distplot(dataset[\"a2\"], color='red', bins=100, ax=axs[1])\n",
    "# fig.savefig(\"imgs/dist_frac_a2_freq.png\")"
   ]
  },
  {
   "source": [
    "Distribution Plots\n",
    "\n",
    "![cat_frac_a2_freq](imgs/dist_r1_r2_freq.png)\n",
    "![cat_frac_a2_freq](imgs/dist_frac_a2_freq.png)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Before moving forward, we can look at the output or labels/targets. These are the 'eta c' and 'Nc' columns."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(ncols=2, figsize=(12,6))\n",
    "# sns.distplot(dataset[\"eta c\"], color='red', bins=100, ax=axs[0])\n",
    "# sns.distplot(dataset[\"Nc\"], color='red', bins=100, ax=axs[1])\n",
    "# fig.savefig(\"imgs/dist_outputs.png\")"
   ]
  },
  {
   "source": [
    "![dist_outputs](imgs/dist_outputs.png)\n",
    "\n",
    "From this we can see that the target distributions are skewed towards the lower end of the specturm. Rescaling 'Nc' between -1 and 1 will result in more dense data with larger outliers. Using 'eta c' as the output may be better. Before emoving on, we should plot all the variables agains one another (see Data.png)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes long to plot -> see saved images.\n",
    "\n",
    "# sns.pairplot(dataset[['r1', '2a2', 'r2', 'frac', 'Nc', 'eta c']], diag_kind='kde')"
   ]
  },
  {
   "source": [
    "Pairplot of the data.\n",
    "\n",
    "![Pairplot](imgs/Data.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Unfortunatley, the data does not show any direct correlations and a hear map is not very useful here. When looking at the data it is useful to remember that the simulations used the inputs to predict 'eta c' or 'Nc' when percolation in the system will occur. We will look at the expanded data and the output now."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig=sns.pairplot(dataset[['a1', 'b1', 'a2', 'b2', 'area1', 'area2', 'eta c']], diag_kind='kde')\n",
    "# fig.savefig(\"imgs/expanded_pairplot.png\")"
   ]
  },
  {
   "source": [
    "The expanded pairplot.\n",
    "\n",
    "![Pairplot](imgs/expanded_pairplot.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Now the range of each feature is different and some of them are unbalanced. The features can be standardized between -1 and 1 with a 0 mean. This will be useful to prevent one feature from dominating the weight updates. We can account for the unbalancing by ommiting some data and/or feature selection. However, lets try PCA first and see how the problem looks. \n",
    "\n",
    "NOTE: The targets 'eta c' and 'Nc' have been scaled."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaled_dataset = dataset.copy()\n",
    "scaled_dataset['r1'] = StandardScaler().fit_transform(dataset['r1'].values.reshape(-1,1))\n",
    "scaled_dataset['r2'] = StandardScaler().fit_transform(dataset['r2'].values.reshape(-1,1))\n",
    "scaled_dataset['frac'] = StandardScaler().fit_transform(dataset['frac'].values.reshape(-1,1))\n",
    "scaled_dataset['2a2'] = StandardScaler().fit_transform(dataset['a2'].values.reshape(-1,1))\n",
    "scaled_dataset['eta c'] = StandardScaler().fit_transform(dataset['eta c'].values.reshape(-1,1))\n",
    "scaled_dataset['Nc'] = StandardScaler().fit_transform(dataset['Nc'].values.reshape(-1,1))\n",
    "\n",
    "#account for added features\n",
    "scaled_dataset[\"a1\"]=1/2\n",
    "scaled_dataset[\"b1\"]=scaled_dataset[\"a1\"]/scaled_dataset[\"r1\"]\n",
    "scaled_dataset[\"a2\"]=scaled_dataset[\"2a2\"]/2\n",
    "scaled_dataset[\"b2\"]=scaled_dataset[\"a2\"]/scaled_dataset[\"r2\"]\n",
    "dataset[\"area1\"]=dataset[\"a1\"]*dataset[\"b1\"]*np.pi\n",
    "dataset[\"area2\"]=dataset[\"a2\"]*dataset[\"b2\"]*np.pi\n",
    "scaled_dataset[\"TotalArea\"] = StandardScaler().fit_transform(dataset['TotalArea'].values.reshape(-1,1))\n",
    "scaled_dataset[\"RatioTotalArea\"] = StandardScaler().fit_transform(dataset['RatioTotalArea'].values.reshape(-1,1))\n",
    "\n",
    "scaled_dataset.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_dataset.head()"
   ]
  },
  {
   "source": [
    "We can visualise the (original) features and targets again to understand the scaling."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(ncols=2, figsize=(12,6))\n",
    "# sns.distplot(scaled_dataset[\"eta c\"], color='red', bins=100, ax=axs[0])\n",
    "# sns.distplot(scaled_dataset[\"Nc\"], color='red', bins=100, ax=axs[1])\n",
    "# fig.savefig(\"scaled_labels.png\")\n",
    "\n",
    "# fig, axs = plt.subplots(ncols=2, figsize=(12,6))\n",
    "# sns.distplot(scaled_dataset[\"r1\"], color='red', bins=1000, ax=axs[0])\n",
    "# sns.distplot(scaled_dataset[\"r2\"], color='red', bins=1000, ax=axs[1])\n",
    "# fig.savefig(\"scaled_r_features.png\")\n",
    "\n",
    "# fig, axs = plt.subplots(ncols=2, figsize=(12,6))\n",
    "# sns.distplot(scaled_dataset[\"frac\"], color='red', bins=100, ax=axs[0])\n",
    "# sns.distplot(scaled_dataset[\"a2\"], color='red', bins=100, ax=axs[1])\n",
    "# fig.savefig(\"scaled_frac_a2.png\")\n",
    "\n",
    "# fig, axs = plt.subplots(ncols=2, figsize=(12,6))\n",
    "# sns.distplot(scaled_dataset[\"area1\"], color='red', bins=100, ax=axs[0])\n",
    "# sns.distplot(scaled_dataset[\"area2\"], color='red', bins=100, ax=axs[1])\n",
    "# fig.savefig(\"imgs/scaled_areas.png\")"
   ]
  },
  {
   "source": [
    "The scaled data.\n",
    "\n",
    "![scaled_r](imgs/scaled_r_features.png)\n",
    "![scaled_frac](imgs/scaled_frac_a2.png)\n",
    "![scaled_labels](imgs/scaled_labels.png)\n",
    "![scaled_areas](imgs/scaled_areas.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Here you can clearly see that r1 and r2 have a bias with large outliers. frac and a2 have pretty uniform distributions and can be handled pretty easily by ommiting samples at the lower and higher end. For now I do not think this is necessary because the difference in the number of samples is not huge and will effect the other distributions. eta c still looks like a better label, with a smaller range, fewer outliers and less of a bias than Nc. The area values are positive and kept between 0 and 1 making them a possible set of inputs alongside the fractions. This would give the input feature vector as {area1,area2,frac} and labels as given.\n",
    "\n",
    "We can look at the covariance matrix of the data, but we know that they were slected relatively independantley. The pairplot and covariance matrix will allow us to check for any easyier representations of the data. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_dataset.cov()"
   ]
  },
  {
   "source": [
    "From the orginal data, eta c seems to have a negative linear tendency with r1, r2 and 2a2, however its relationship with 2a2 is weak. From the expanded data you can see the strongest correlation with b1, followed by the TotalArea and RatioTotalArea. If we use b1, we will have to use a2/2a2, and b2 with frac in the input space. However, the axis have weak correlations with eta c. If we look at the total area, we see a stong correlation with eta c. This may indicate a good input variable to use. Options can include:\n",
    "- {frac, TotalArea}\n",
    "- {RatioTotalArea}\n",
    "- {TotalArea}\n",
    "- {frac, RatioTotalArea}\n",
    "\n",
    "RatioTotalArea has the frac encoded into it while total area does not. It is probably more advisable to use {frac, RatioTotalArea}. I would choose this because there is a strong correlation, RatioTotalArea has area1 and area2 and, thus, r1 and r2 encoded in it. It also has frac, but I may think that having frac explicitly defined in the input is better.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Checking the Areas\n",
    "We will look at a total area and fractional-added total area columns. The data below shows some kind of shape with eta c and the 2 values. This is interesting and may show that using this in the model is useful. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig=sns.pairplot(scaled_dataset[['TotalArea', 'RatioTotalArea', 'eta c']], diag_kind='kde')\n",
    "# fig.savefig(\"imgs/totalareas_pairplot.png\")"
   ]
  },
  {
   "source": [
    "Pairplot of areas, areas including frac and eta c.\n",
    "\n",
    "![total_areas](imgs/totalareas_pairplot.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## r1 = r2\n",
    "\n",
    "Here we will look at keeping some variables constant or in a range. For instance, we can keep the frac constant and see how eta c varies with the RatioTotalArea. Another option could be keeping r1 and r2 constant and plotting eta c vs 2 other variables.\n",
    "\n",
    "We notice similar surfaces when we keep r1 and r2 the same and we notice similar surfaces when r1 and r2 are differet. Could this indicate 2 models? "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the data according to some criterion or criteria\n",
    "val = 500\n",
    "selection_dataset = dataset.copy()\n",
    "select_1 = selection_dataset[dataset[\"frac\"]<2]\n",
    "select_2 = select_1[select_1[\"r2\"]==val]\n",
    "select_3 = select_2[select_2[\"r1\"]==val]\n",
    "final_selection = select_3.copy()\n",
    "\n",
    "#__________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
    "# scale the data\n",
    "final_selection['r1'] = StandardScaler().fit_transform(final_selection['r1'].values.reshape(-1,1))\n",
    "final_selection['r2'] = StandardScaler().fit_transform(final_selection['r2'].values.reshape(-1,1))\n",
    "final_selection['frac'] = StandardScaler().fit_transform(final_selection['frac'].values.reshape(-1,1))\n",
    "final_selection['2a2'] = StandardScaler().fit_transform(final_selection['a2'].values.reshape(-1,1))\n",
    "final_selection['eta c'] = StandardScaler().fit_transform(final_selection['eta c'].values.reshape(-1,1))\n",
    "final_selection['Nc'] = StandardScaler().fit_transform(final_selection['Nc'].values.reshape(-1,1))\n",
    "\n",
    "#account for added features\n",
    "final_selection['a1'] = StandardScaler().fit_transform(final_selection['a1'].values.reshape(-1,1))\n",
    "final_selection['a2'] = StandardScaler().fit_transform(final_selection['b1'].values.reshape(-1,1))\n",
    "final_selection['a2'] = StandardScaler().fit_transform(final_selection['a2'].values.reshape(-1,1))\n",
    "final_selection['b2'] = StandardScaler().fit_transform(final_selection['b2'].values.reshape(-1,1))\n",
    "final_selection['area1'] = StandardScaler().fit_transform(final_selection['area1'].values.reshape(-1,1))\n",
    "final_selection['area2'] = StandardScaler().fit_transform(final_selection['area2'].values.reshape(-1,1))\n",
    "final_selection[\"TotalArea\"] = StandardScaler().fit_transform(final_selection['TotalArea'].values.reshape(-1,1))\n",
    "final_selection[\"RatioTotalArea\"] = StandardScaler().fit_transform(final_selection['RatioTotalArea'].values.reshape(-1,1))\n",
    "\n",
    "#__________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
    "xaxis_label = \"RatioTotalArea\"\n",
    "yaxis_label = \"eta c\"\n",
    "\n",
    "title = \"Comparing {x} and {y}\".format(x=xaxis_label, y=yaxis_label)\n",
    "# plt.scatter(x=np.arange(len(final_selection)), y=final_selection[yaxis_label], marker='.')\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.scatter(x=final_selection[xaxis_label], y=final_selection[yaxis_label], marker='.')\n",
    "plt.ylabel(yaxis_label)\n",
    "plt.xlabel(xaxis_label)\n",
    "plt.title(title)\n",
    "# fig.savefig(\"imgs/edited_data/r1_r2_{}.png\".format(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "z = \"eta c\"\n",
    "y = \"frac\"\n",
    "x = \"2a2\"\n",
    "\n",
    "zdata =  select_3[z]\n",
    "ydata =  select_3[y]\n",
    "xdata = select_3[x]\n",
    "# ax.scatter3D(xdata, ydata, zdata, c=zdata, cmap='Reds');\n",
    "# ax.plot3D(xdata, ydata, zdata, 'gray')\n",
    "ax.scatter3D(xdata, ydata, zdata);\n",
    "plt.ylabel(y)\n",
    "plt.xlabel(x)\n",
    "# fig.savefig(\"imgs/edited_data/3d_r1_r2_{}.png\".format(val))\n",
    "# plt.zlabel(\"eta c\")"
   ]
  },
  {
   "source": [
    "After plotting the data with r1=r2 you can see similar shapes in the 3d plots were {x,y,z}={2a2,frac,eta c}. This is encouraging because it shows that for the data where r1 and r2 are equal, we can create a ML algo to learn that 3D shape, hopefully producing the correct eta c output. Similarly, if you plot {x,y}={RatioTotalArea, eta c} you can similar curves for each r1=r2 pair. The data may be slightly differently spread, but this is okay. Possibly doing a 3D surface like for the other will allow a surface ot be learnt, however, the {x,y,z}={2a2,frac,eta c} plot shows that this may be irrelevant for the r1=r2 values, because we can learn from the given data without need for the expanded data. Now we need to check for the data where r1!=r2. Previously I looked at it and a simialr shape could be seen (but with more fluctation between the values). However, I cannot remember if the shape was simialr to the r1=r2 shape. We will check these below. \n",
    "\n",
    "The 3D curve shows how eta c varies with 2a2 and frac. Out of the entire data set, r1=r2 accounts for 2892 of 57840, or 5%. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r1_new_range = single_input_vs_output(final_selection, \"r1\")\n",
    "dataset[dataset[\"r1\"]< dataset[\"r2\"]].count()\n",
    "# dataset[dataset[\"r1\"]<150].count()[\"r1\"]"
   ]
  },
  {
   "source": [
    "## r1 != r2\n",
    "Here, we will do a similar thing to above but for the majority of the data were r1 != r2. We will not look at all of the data, but rather 4 from each. \n",
    "\n",
    "After plotting the data for this,some similar shapes were seen both within a set and across sets with similar r2 values. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3,4]\n",
    "y = [5,10,15,20]\n",
    "z = [2, 6, 4, 8]\n",
    "colors = ['blue', 'green', 'magenta', 'orange']\n",
    "colour_selector = 0\n",
    "\n",
    "fig = plt.figure(figsize=(4,4))\n",
    "ax = plt.axes(projection='3d')\n",
    "for i in range(0, len(x)):\n",
    "    ax.scatter3D(x[i], y[i], z[i], c=colors[colour_selector])\n",
    "    colour_selector+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "\n",
    "colors = ['blue', 'green', 'magenta', 'orange']\n",
    "# colors = ['green', 'blue', 'orange', 'magenta']\n",
    "\n",
    "ii_prev = -1\n",
    "choose_set = -1\n",
    "# this loop runs for each r1, giving r1=x and r2=[x1,x2,x3,x4]\n",
    "for ii in range(0,20):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = plt.axes(projection='3d')\n",
    "    z = \"eta c\"\n",
    "    y = \"frac\"\n",
    "    x = \"2a2\"\n",
    "    # selector needs to go from 0-19 (20 values in r1_range)\n",
    "    if ii_prev==4:\n",
    "        choose_set = 0\n",
    "    else:\n",
    "        choose_set += 1\n",
    "    ii_prev = choose_set\n",
    "\n",
    "    selector = np.arange(5,21,5)-choose_set-1     # gives [5,10,15,20] - choose_set\n",
    "\n",
    "    colour_selector = 0\n",
    "    for i in selector:\n",
    "        # selects the r1 value \n",
    "        val = (r1_range[\"r1\"].to_numpy())[ii] \n",
    "        val2 = (r2_range[\"r2\"].to_numpy())[i]\n",
    "\n",
    "        # selects the data sets\n",
    "        selection_dataset = dataset.copy()\n",
    "        # filters the data to the chosen r1 value (outer loop)\n",
    "        select_1 = selection_dataset[selection_dataset[\"r1\"]==val]\n",
    "        # filters the data to the chosen r2 value (inner loop, so 1 of 4)\n",
    "        select_2 = select_1[select_1[\"r2\"]==val2]\n",
    "        \n",
    "        final_selection = select_2\n",
    "        # scale the data\n",
    "        final_selection['r1'] = StandardScaler().fit_transform(final_selection['r1'].values.reshape(-1,1))\n",
    "        final_selection['r2'] = StandardScaler().fit_transform(final_selection['r2'].values.reshape(-1,1))\n",
    "        final_selection['frac'] = StandardScaler().fit_transform(final_selection['frac'].values.reshape(-1,1))\n",
    "        final_selection['2a2'] = StandardScaler().fit_transform(final_selection['a2'].values.reshape(-1,1))\n",
    "        final_selection['eta c'] = StandardScaler().fit_transform(final_selection['eta c'].values.reshape(-1,1))\n",
    "\n",
    "        # plots the data for each \n",
    "        zdata =  final_selection[z]\n",
    "        ydata =  final_selection[y]\n",
    "        xdata = final_selection[x]\n",
    "        ax.scatter3D(xdata, ydata, zdata, c=colors[colour_selector-1])\n",
    "        colour_selector+=1\n",
    "    \n",
    "    title = \"Comparing r1={x} and r2={y}\".format(x=val, y=str(selector))\n",
    "    plt.ylabel(y)\n",
    "    plt.xlabel(x)\n",
    "    plt.title(title)\n",
    "    fig.savefig(\"/media/nirav/34E0-F309/KTH/Thesis/edited_data/diff r1 and r2/r1_{x}.png\".format(x=val))\n",
    "    plt.close(fig)\n",
    "    # plt.zlabel(\"eta c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.tail(\n",
    ")"
   ]
  },
  {
   "source": [
    "The data distribution is still not good for most, so it seems like using the area is our best bet for now. Going back to the pairplot, we can see that a lot of the data is clumped in the <0.5 area range for both ellipse 1 and 2. Maybe we can try splitting this and only including for the areas <0.5 and testing a trained model on this, to start. \n",
    "\n",
    "Doint this may reduce the dataset substantially because there is not correlation between the two."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_areas_scaled_dataset = scaled_dataset.copy()\n",
    "\n",
    "# drop_areas_scaled_dataset.drop(drop_areas_scaled_dataset[drop_areas_scaled_dataset.area1 > 0.55].index, inplace=True)\n",
    "# drop_areas_scaled_dataset.drop(drop_areas_scaled_dataset[drop_areas_scaled_dataset.area2 > 0.55].index, inplace=True)\n",
    "# drop_areas_scaled_dataset.describe().transpose()\n",
    "\n",
    "# fig, axs = plt.subplots(ncols=2, figsize=(12,6))\n",
    "# sns.distplot(drop_areas_scaled_dataset[\"area1\"], color='red', bins=100, ax=axs[0])\n",
    "# sns.distplot(drop_areas_scaled_dataset[\"area2\"], color='red', bins=100, ax=axs[1])"
   ]
  },
  {
   "source": [
    "## PCA\n",
    "\n",
    "Now we will implement PCA to try and reduce the number of input features. The total number of un expanded features is 4. With the expansion, we get 8. First we will try PCA on the 4, then the 8 and then maybe a selection of the features. For now the labels will be the nonstandardizedd 'eta c'. In the future we can possibly use both 'eta c' and 'Nc' or their scaled versions. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "features = scaled_dataset.drop(['eta c', 'Nc', 'Nc Std. Dev', 'a1', 'b1', 'a2', 'b2'], axis=1)\n",
    "expanded_features = scaled_dataset.drop(['eta c', 'Nc', 'Nc Std. Dev',], axis=1)\n",
    "labels = scaled_dataset['eta c']\n",
    "\n",
    "components = 2\n",
    "components_cols = ['PC 1', 'PC 2']\n",
    "X = features\n",
    "Y = labels\n",
    "\n",
    "pca = PCA(n_components=components)\n",
    "pca_data = pca.fit_transform(X.values)\n",
    "\n",
    "principalDf = pd.DataFrame(data = pca_data, columns = components_cols)\n",
    "\n",
    "pcaDF = pd.concat([principalDf, Y], axis = 1)\n",
    "pcaDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(ncols=1, figsize=(12,6))\n",
    "# sns.distplot(scaled_dataset[\"frac\"], color='red', bins=100, ax=axs[0])\n",
    "# sns.distplot(scaled_dataset[\"a2\"], color='red', bins=100, ax=axs[1])\n",
    "fig = sns.pairplot(pcaDF[['PC 1', 'PC 2', 'eta c']], diag_kind='kde')\n",
    "fig.savefig(\"imgs/PCA.png\")\n"
   ]
  },
  {
   "source": [
    "The PCA data has been plotted in a pair plot to see how the new components relate to the output. \n",
    "\n",
    "![pca](imgs/PCA.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "An attempt with 1 component."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = 1\n",
    "components_cols = ['PC 1']\n",
    "X = features\n",
    "Y = labels\n",
    "\n",
    "pca = PCA(n_components=components)\n",
    "pca_data = pca.fit_transform(X.values)\n",
    "\n",
    "principalDf = pd.DataFrame(data = pca_data, columns = components_cols)\n",
    "\n",
    "pcaDF = pd.concat([principalDf, Y], axis = 1)\n",
    "pcaDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.pairplot(pcaDF[['PC 1', 'eta c']], diag_kind='kde')\n",
    "fig.savefig(\"OnePCA.png\")"
   ]
  },
  {
   "source": [
    "Lastly, we will store the dataframes to csv files to be used in other notebooks."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_dataset.to_csv(\"NormalizedData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaDF.to_csv(\"PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}