{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GenDatasets.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1IjZARY1zynmfu7wKFsceoRZrRpQBku1L","authorship_tag":"ABX9TyNL7n1i6dlPn3qb2ioMxwuO"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"OHLSa2Mq7TwB"},"source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MaxAbsScaler\n","import seaborn as sns\n","sns.set()\n","\n","seed_val = 2021\n","np.random.seed(seed_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tPIxGRRo75Ul"},"source":["def split_data(dataset, seed, train_ratio=0.6, shuffle=True, highest_at_top=False, no_outliers=False):\n","    if shuffle:\n","        dataset = dataset.sample(frac=1, random_state=seed)#.reset_index(drop=True)\n","    \n","    if highest_at_top:\n","        df = dataset.copy()\n","        sub_115 = df.drop( df[ df[\"eta c\"]>1.0 ].index)\n","        big_115 = df[df[\"eta c\"] > 1.0]\n","        # shuffle eta c < 1.15 data\n","        sub_115 = sub_115.sample(frac=1, random_state=seed)\n","        # join the data\n","        x = big_115.append(sub_115)\n","\n","        train_dataset = x[0:int(len(x)*train_ratio)]\n","        test_dataset = dataset.drop(train_dataset.index)\n","        \n","        if shuffle:\n","          train_dataset = train_dataset.sample(frac=1, random_state=seed)#.reset_index(drop=True)\n","          test_dataset = test_dataset.sample(frac=1, random_state=seed)#.reset_index(drop=True)\n","        \n","        return train_dataset, test_dataset\n","\n","    elif no_outliers:\n","        df = dataset.copy()\n","        sub_115 = df[df[\"eta c\"]<=1.15 ]\n","        big_115 = df[df[\"eta c\"] > 1.15]\n","        num_samples = len(df)*train_ratio\n","        sub_115 = sub_115.sample(frac=1, random_state=seed)\n","        train_dataset = sub_115[0:int(num_samples)]\n","        test_dataset = big_115.append(sub_115.drop(train_dataset.index))\n","\n","        return train_dataset, test_dataset\n","\n","    \n","    train_dataset = dataset.sample(frac=train_ratio, random_state=0)\n","    test_dataset = dataset.drop(train_dataset.index)\n","\n","    return train_dataset, test_dataset\n","\n","def split_data_with_vali(dataset, seed, train_ratio=0.6, vali_ratio=0.2, shuffle=True, highest_at_top=False, no_outliers=False):    \n","    train_dataset = dataset.sample(frac=train_ratio, random_state=0)\n","    test_and_vali = dataset.drop(train_dataset.index)\n","    vali_dataset = test_and_vali.sample(frac=vali_ratio/(1-train_ratio), random_state=0)\n","    test_dataset = test_and_vali.drop(vali_dataset.index)\n","\n","    return train_dataset, vali_dataset, test_dataset\n","\n","def add_bias(data):\n","    N1 = np.shape(data)[0]\n","    N2 = np.shape(data)[1]\n","    a = -1*np.ones((N1,N2+1))\n","    a[:,:-1] = data\n","    return a\n","\n","def add_noise(dataset, label_column=4, noise_var=0.01, input_n=False, output_n=False, augment=False, aug_frac=0.5, n_c_limit=1.4):\n","    \"\"\" Called on DATAFRAME training data. \"\"\"\n","    dataset2 = dataset.copy()\n","    dataset = dataset[dataset[\"eta c\"]<n_c_limit]\n","    print(\"# samples with restriction:\", len(dataset))\n","\n","    if augment:\n","      dataset = dataset.sample(frac=aug_frac, random_state=7)\n","      # original features and labels\n","      features2 = dataset2.to_numpy()[:,0:label_column]\n","      labels2 = np.reshape(dataset2.to_numpy()[:,label_column], (-1,1))\n","\n","    # to add noise to\n","    features = dataset.to_numpy()[:,0:label_column]\n","    labels = np.reshape(dataset.to_numpy()[:,label_column], (-1,1))\n","\n","    if input_n:\n","        noise = np.reshape(np.random.normal(0,noise_var,np.shape(features)[0]*np.shape(features)[1]),(np.shape(features)[0],np.shape(features)[1]))\n","        features = features + noise \n","\n","    if output_n:\n","        noise = np.reshape(np.random.normal(0,noise_var,np.shape(labels)[0]*np.shape(labels)[1]),(np.shape(labels)[0],np.shape(labels)[1]))\n","        labels = labels + noise\n","    \n","    if augment:\n","      # if augmenting original data\n","      print(\"Train # samples:\", len(features2))\n","      print(\"# samples with noise:\", len(dataset))\n","      features3 = np.vstack((features2, features))\n","      labels3 = np.vstack((labels2, labels))\n","      print(\"New # of samples:\", len(features3))\n","      return features3, labels3\n","    else:\n","      return features, labels\n","\n","def check_eta_range(dataset, nums=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3,1.4]):\n","    # nums = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3,1.4]\n","    less_than_percents = []\n","    in_range_percents = []\n","    x_ranges = []\n","\n","    # lim_dataset[ (lim_dataset[\"b1\"]<0.1) & (lim_dataset[\"RatioTotalArea\"]<0.2) ]\n","\n","    for i in range(len(nums)):\n","        less_than_percents.append( ((len(dataset[dataset[\"eta c\"] <= nums[i]]))/len(dataset))*100)\n","        in_range_percents.append( (len(dataset[ (dataset[\"eta c\"]<nums[i]) & (dataset[\"eta c\"] >= nums[i-1]) ])/len(dataset))*100 )\n","        x_ranges.append(\"{one}<EtaC<{two}\".format(one=nums[i-1], two=nums[i]) )\n","\n","    nums.pop(0)\n","    less_than_percents.pop(0)\n","    in_range_percents.pop(0)\n","    x_ranges.pop(0)\n","\n","    fig = plt.figure()\n","    fig, ax = plt.subplots(ncols=2, figsize=(15,8))\n","\n","    ax[0].scatter(nums, less_than_percents)\n","    ax[0].set_ylabel(\"Percentage\")\n","    ax[0].set_xlabel(\"Eta c\")\n","    ax[0].set_title(\"Percentage of data with Eta C less than x\")\n","    for i in range(len(less_than_percents)):\n","        if less_than_percents[i] > 99.0:\n","            ax[0].scatter(nums[i], less_than_percents[i], c=\"red\", label=\">99%\")\n","    ax[0].legend()\n","\n","\n","\n","    ax[1].scatter(x_ranges, in_range_percents)\n","    ax[1].set_ylabel(\"Percentage\")\n","    ax[1].set_xlabel(\"Eta c\")\n","    ax[1].set_title(\"Percentage of data with Eta C within range\")\n","    for i in range(len(in_range_percents)):\n","        if sum(in_range_percents[0:i])>99:\n","            ax[1].scatter(x_ranges[i], in_range_percents[i], c=\"red\", label=\"sum>99%\")\n","    ax[1].legend()\n","\n","    plt.xticks(rotation='vertical')\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZEzvLF0qPAm9"},"source":["def unique_values(dataset, input_column, output_column=\"eta c\", plot=False, output=False):\n","    reduced_dataset = dataset.drop_duplicates(subset=input_column)\n","    # data_range = reduced_dataset[input_column].to_numpy()\n","    if output:\n","        print(\"{col} range is: {rng}    (output is {out})\".format(col=input_column, rng=len(reduced_dataset), out=output_column))\n","\n","    if plot:\n","        ax1 = reduced_dataset.plot.scatter( x=input_column,\n","                        y=output_column,\n","                        c='DarkBlue')\n","        return ax1\n","        # return sns.scatterplot(data=reduced_dataset, x=input_column, y=output_column)\n","    \n","    return reduced_dataset[[input_column, output_column]]\n","\n","def add_input_noise(dataset, noise_perc=0.01):\n","  print(\"ADDING {}% INPUT NOISE.\".format(noise_perc*100))\n","  # adds noise to b1 and a2\n","  pd.options.mode.chained_assignment = None  # default='warn'\n","\n","  # dataset2 = dataset[dataset[\"eta c\"] < 0.07]\n","  dataset2 = dataset.copy()\n","\n","  # add noise to b1\n","  names = [x for x in dataset.columns]\n","  df_b = pd.DataFrame(columns=names)\n","  b1_unique = list(unique_values(dataset, \"b1\")[\"b1\"])\n","  for i in range(0, len(b1_unique)):\n","    temp = dataset2[dataset2[\"b1\"] == b1_unique[i]]\n","    noise_var = b1_unique[i]*noise_perc\n","    noise = np.reshape(np.random.normal(0,noise_var,len(temp)), (-1,1))\n","    temp[\"b1\"] = temp[[\"b1\"]] + noise\n","    df_b = df_b.append(temp)\n","  \n","  # if len(df_b) == len(dataset):\n","  #   print(\"All {} b1 have noise. Moving on to a2.\".format(len(df_b)))\n","\n","  # add noise to a2\n","  df_a = pd.DataFrame(columns=names)\n","  a2_unique = list(unique_values(dataset, \"a2\")[\"a2\"])\n","  for i in range(0, len(a2_unique)):\n","    temp = df_b[df_b[\"a2\"] == a2_unique[i]]\n","    noise_var = a2_unique[i]*noise_perc\n","    noise = np.reshape(np.random.normal(0,noise_var,len(temp)), (-1,1))\n","    temp[\"a2\"] = temp[[\"a2\"]] + noise\n","    df_a = df_a.append(temp)\n","  \n","  # if len(df_a) == len(dataset):\n","  #   print(\"All {} a2 have noise.\".format(len(df_a)))\n","  \n","  # add noise to b2\n","  df_b2 = pd.DataFrame(columns=names)\n","  b2_unique = list(unique_values(dataset, \"b2\")[\"b2\"])\n","  for i in range(0, len(b2_unique)):\n","    temp = df_a[df_a[\"b2\"] == b2_unique[i]]\n","    noise_var = b2_unique[i]*noise_perc\n","    noise = np.reshape(np.random.normal(0,noise_var,len(temp)), (-1,1))\n","    temp[\"b2\"] = temp[[\"b2\"]] + noise\n","    df_b2 = df_b2.append(temp)\n","  \n","  # if len(df_b2) == len(dataset):\n","  #   print(\"All {} b2 have noise.\".format(len(df_b2)))\n","\n","  # add noise to a1\n","  df_a1 = df_b2\n","  noise = np.reshape(np.random.normal(0,0.5*noise_perc,len(df_a1)), (-1,1))\n","  df_a1[\"a1\"] = df_a1[[\"a1\"]] + noise\n","  # if len(df_a1) == len(dataset):\n","  #   print(\"All {} a1 have noise.\".format(len(df_a1)))\n","\n","  # add noise to frac\n","  df_f = pd.DataFrame(columns=names)\n","  frac_unique = list(unique_values(dataset, \"frac\")[\"frac\"])\n","  for i in range(0, len(frac_unique)):\n","    temp = df_a1[df_a1[\"frac\"] == frac_unique[i]]\n","    noise_var = frac_unique[i]*noise_perc\n","    noise_var = 0\n","    noise = np.reshape(np.random.normal(0,noise_var,len(temp)), (-1,1))\n","    temp[\"frac\"] = temp[[\"frac\"]] + noise\n","    df_f = df_f.append(temp)\n","\n","  # if len(df_f) == len(dataset):\n","  #   print(\"All {} frac have noise.\".format(len(df_f)))\n","\n","  df_a = df_f\n","  # noisey to RTA\n","  df_a[\"area1\"]=df_a[\"a1\"]*df_a[\"b1\"]*np.pi\n","  df_a[\"area2\"]=df_a[\"a2\"]*df_a[\"b2\"]*np.pi\n","  df_a[\"TotalArea\"] = df_a[\"area1\"] + df_a[\"area2\"]\n","  df_a[\"RatioTotalArea\"] = df_a[\"area1\"]*(1-df_a[\"frac\"]) + df_a[\"area2\"]*(df_a[\"frac\"])\n","\n","  # print(\"All RTA have noise. Returning dataset.\")\n","  print(\"# samples with input noise:\", len(df_a))\n","  print()\n","  return df_a.sort_index()\n","\n","def add_output_noise(dataset, ds_no_noise, noise_perc=0.01, noise_frac=1.0):\n","    \"\"\" Called on DATAFRAME training data. \"\"\"\n","    print(\"ADDING {}% OUTPUT NOISE.\".format(noise_perc*100))\n","    dataset2 = dataset.copy()\n","\n","    # dataset = dataset.sample(frac=noise_frac, random_state=7)\n","    rta_signs = np.reshape(np.array(np.sign(dataset[[\"RatioTotalArea\"]] - ds_no_noise[[\"RatioTotalArea\"]])), (-1,1))*(-1)\n","    noise_ints = abs(np.reshape(np.random.normal(0, noise_perc, len(dataset)), (-1,1)))*rta_signs\n","    # noise_ints = np.reshape(np.random.normal(0, noise_perc, len(dataset)), (-1,1))\n","    dataset[\"eta c\"] = dataset[[\"eta c\"]]*(1+noise_ints) \n","\n","    print(\"# samples with output noise:\", len(dataset))\n","    print()\n","    return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pwLA58dJkABY"},"source":["def scale_data(data, label_name, scale_label=False):\n","  scaled_dataset = data.copy()\n","  scaled_dataset['b1'] = MaxAbsScaler().fit_transform(data['b1'].values.reshape(-1,1))\n","  scaled_dataset['a2'] = MaxAbsScaler().fit_transform(data['a2'].values.reshape(-1,1))\n","  scaled_dataset['b2'] = MaxAbsScaler().fit_transform(data['b2'].values.reshape(-1,1))\n","  scaled_dataset['RatioTotalArea'] = MaxAbsScaler().fit_transform(data['RatioTotalArea'].values.reshape(-1,1))\n","  scaled_dataset['frac'] = MaxAbsScaler().fit_transform(data['frac'].values.reshape(-1,1))\n","  if scale_label:\n","    scaled_dataset[label_name] = MaxAbsScaler().fit_transform(data[label_name].values.reshape(-1,1))\n","  return scaled_dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F88kzlfpFjRe"},"source":["def change_num_samples(dataset, ratio_to_keep=0.5, condition=0.15, seed=97, label_name='Nc'):\n","  names = [x for x in dataset.columns]\n","  df_data = pd.DataFrame(columns=names)\n","  df_data = df_data.append(dataset[dataset[label_name]>condition])\n","  df_leftover = pd.DataFrame(columns=names)\n","  \n","  if label_name == 'eta c':\n","    for i in range(0, int(100*condition)+5, 5):\n","      lim_df = dataset[(dataset[label_name] >= (i-5)/100) & (dataset[label_name] <= i/100)]\n","      sampled_df = lim_df.sample(frac=ratio_to_keep, random_state=seed)\n","      remain_df = lim_df.drop(sampled_df.index)\n","\n","      df_data = df_data.append(sampled_df)\n","      df_leftover = df_leftover.append(remain_df)\n","\n","      print(\"{x} / {y}\".format(x=len(sampled_df), y=len(remain_df)))\n","  elif label_name == 'Nc':\n","      for i in range(0, int(condition)+5, 5):\n","        lim_df = dataset[(dataset[label_name] >= i) & (dataset[label_name] <= i+5)]\n","        sampled_df = lim_df.sample(frac=ratio_to_keep, random_state=seed)\n","        remain_df = lim_df.drop(sampled_df.index)\n","\n","        df_data = df_data.append(sampled_df)\n","        df_leftover = df_leftover.append(remain_df)\n","\n","        # print(\"{x} / {y}\".format(x=len(sampled_df), y=len(remain_df)))\n","\n","  return df_data, df_leftover"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"parLavZ2HMZe"},"source":["def check_representation(train_dataset, validation_dataset, test_dataset, label_name):\n","  # sort the data to check representation\n","  sorted_train = train_dataset.sort_index()\n","  sorted_test = test_dataset.sort_index()\n","\n","  # sort the data to check representation\n","  sorted_train = train_dataset.sort_index()\n","  sorted_vali = validation_dataset.sort_index()\n","  sorted_test = test_dataset.sort_index()\n","\n","  # check representation\n","  fig = plt.figure()\n","  fig, ax = plt.subplots(ncols=3, nrows=2, figsize=(15,12))\n","\n","  ax[0][0].scatter(x=np.arange(len(sorted_train)), y=sorted_train[label_name], marker='.', alpha=0.4)\n","  ax[0][0].set_title(\"Index Sorted Training Data\")\n","  ax[0][1].scatter(x=np.arange(len(sorted_vali)), y=sorted_vali[label_name], marker='.', alpha=0.4)\n","  ax[0][1].set_title(\"Index Sorted Validation Data\")\n","  ax[0][2].scatter(x=np.arange(len(sorted_test)), y=sorted_test[label_name], marker='.', alpha=0.4)\n","  ax[0][2].set_title(\"Index Sorted Testing Data\")\n","  ax[0][0].set_ylabel(label_name)\n","\n","  train1 = sorted_train.sort_values(label_name)\n","  test1 = sorted_test.sort_values(label_name)\n","\n","  y_val = train1[int(len(train1)/2)-1:int(len(train1)/2)][label_name]\n","  ax[1][0].scatter(x=np.arange(len(train_dataset)), y=train1[label_name], marker='.', alpha=0.4)\n","  # vertical line\n","  ax[1][0].plot([len(train_dataset)/2, len(train_dataset)/2], [0,train_dataset[label_name].max()], '--', alpha=0.4, color=\"red\")\n","  # horizontal line\n","  ax[1][0].plot([0, len(train_dataset)], [y_val,y_val], '--',  alpha=0.4, color=\"red\")\n","  ax[1][0].set_title(\"Label Sorted Training Data\")\n","\n","  sorted_ds=sorted_vali.sort_values(label_name)\n","  y_val = sorted_ds[int(len(sorted_ds)/2)-1:int(len(sorted_ds)/2)][label_name]\n","  ax[1][1].scatter(x=np.arange(len(sorted_ds)), y=sorted_ds[label_name], marker='.', alpha=0.4)\n","  # vertical line\n","  ax[1][1].plot([len(sorted_ds)/2, len(sorted_ds)/2], [0,sorted_ds[label_name].max()], '--',  alpha=0.4, color=\"red\")\n","  # horizontal line\n","  ax[1][1].plot([0, len(sorted_ds)], [y_val,y_val], '--',  alpha=0.4, color=\"red\")\n","  ax[1][1].set_title(\"Label Sorted Train Data\")\n","\n","  sorted_ds=sorted_test.sort_values(label_name)\n","  y_val = sorted_ds[int(len(sorted_ds)/2)-1:int(len(sorted_ds)/2)][label_name]\n","  ax[1][2].scatter(x=np.arange(len(sorted_ds)), y=sorted_ds[label_name], marker='.', alpha=0.4)\n","  # vertical line\n","  ax[1][2].plot([len(sorted_ds)/2, len(sorted_ds)/2], [0,sorted_ds[label_name].max()], '--',  alpha=0.4, color=\"red\")\n","  # horizontal line\n","  ax[1][2].plot([0, len(sorted_ds)], [y_val,y_val], '--',  alpha=0.4, color=\"red\")\n","  ax[1][2].set_title(\"Label Sorted Test Data\")\n","  ax[1][0].set_ylabel(label_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iQHedV_Ff35g"},"source":["def check_label_range(dataset, name, nums=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3,1.4], label='eta c'):\n","    # nums = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3,1.4]\n","    less_than_percents = []\n","    in_range_percents = []\n","    x_ranges = []\n","\n","    # lim_dataset[ (lim_dataset[\"b1\"]<0.1) & (lim_dataset[\"RatioTotalArea\"]<0.2) ]\n","\n","    for i in range(len(nums)):\n","        less_than_percents.append( ((len(dataset[dataset[label] <= nums[i]]))/len(dataset))*100)\n","        in_range_percents.append( (len(dataset[ (dataset[label]<nums[i]) & (dataset[label] >= nums[i-1]) ])/len(dataset))*100 )\n","        x_ranges.append(\"{one}<{label}<{two}\".format(one=nums[i-1], label=label, two=nums[i]) )\n","\n","    nums.pop(0)\n","    less_than_percents.pop(0)\n","    in_range_percents.pop(0)\n","    x_ranges.pop(0)\n","\n","    fig = plt.figure()\n","    fig, ax = plt.subplots(ncols=2, figsize=(15,8))\n","\n","    ax[0].scatter(nums, less_than_percents)\n","    ax[0].set_ylabel(\"Percentage\")\n","    ax[0].set_xlabel(label)\n","    ax[0].set_title(\"Percentage of {name} data with {label} less than x\".format(name=name, label=label))\n","    for i in range(len(less_than_percents)):\n","        if less_than_percents[i] > 99.0:\n","            ax[0].scatter(nums[i], less_than_percents[i], c=\"red\", label=\">99%\")\n","    ax[0].legend()\n","\n","\n","\n","    ax[1].scatter(x_ranges, in_range_percents)\n","    ax[1].set_ylabel(\"Percentage\")\n","    ax[1].set_xlabel(label)\n","    ax[1].set_title(\"Percentage of {name} data with {label} within range\".format(name=name, label=label))\n","    for i in range(len(in_range_percents)):\n","        if sum(in_range_percents[0:i])>99:\n","            ax[1].scatter(x_ranges[i], in_range_percents[i], c=\"red\", label=\"sum>99%\")\n","    ax[1].legend()\n","\n","    plt.xticks(rotation='vertical')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tFvc_fSL7uB0"},"source":["uploaded = pd.read_csv(\"AdditionalStuff.csv\")\n"," \n","dataset = uploaded.copy()\n","dataset.pop(\"Unnamed: 0\")\n","\n","label_name = 'Nc'\n","in_features = 5\n","out_nodes = 1\n","\n","# split the data\n","train_dataset, validation_dataset, test_dataset = split_data_with_vali(dataset.copy(), seed_val, train_ratio=0.8, vali_ratio=0.1, shuffle=False, highest_at_top=False, no_outliers=False)\n","print(\"# of samples :\", len(dataset))\n","print(\"# of starting train samples:\", len(train_dataset))\n","print(\"# of starting vali samples:\", len(validation_dataset))\n","print(\"# of starting test samples:\", len(test_dataset))\n","print()\n","\n","# If noise to be added to data.\n","# Currently adds NO NOISE because this was not seen as useful. \n","# for adding noise to b1 and a2\n","train_dataset_no_noise = train_dataset.copy()\n","in_noisey_train = add_input_noise(train_dataset, noise_perc=0.0)\n","all_noisey_train = add_output_noise(in_noisey_train, train_dataset_no_noise, noise_perc=0.0, noise_frac=1.0)\n","# append the data without noise\n","all_noisey_train = all_noisey_train.sample(frac=0.0, random_state=9)\n","all_train = train_dataset_no_noise.append(all_noisey_train)\n"," \n","print(\"# of Training Samples: \", len(train_dataset))\n","print(\"# of Test Samples: \", len(test_dataset))\n","\n","# select which data to use\n","train_dataset = train_dataset[[\"b1\", \"a2\", \"b2\", \"RatioTotalArea\", \"frac\", label_name]]\n","validation_dataset = validation_dataset[[\"b1\", \"a2\", \"b2\", \"RatioTotalArea\", \"frac\", label_name]]\n","test_dataset = test_dataset[[\"b1\", \"a2\", \"b2\", \"RatioTotalArea\", \"frac\", label_name]]\n"," \n","# # sort the data to check representation\n","sorted_train = train_dataset.sort_index()\n","sorted_vali = validation_dataset.sort_index()\n","sorted_test = test_dataset.sort_index()\n"," \n","check_representation(train_dataset, validation_dataset, test_dataset, label_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"enYvaA1af71o"},"source":["check_label_range(train_dataset.copy(), name='Train', nums=[1,5,10,15,20,25,50,75,100,150,200,250,300,350], label='Nc')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J5wnw8qEf8cg"},"source":["check_label_range(validation_dataset.copy(), name='Validation', nums=[1,5,10,15,20,25,50,75,100,150,200,250,300,350], label='Nc')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vq41JwAnf8Uo"},"source":["check_label_range(test_dataset.copy(), name='Test', nums=[1,5,10,15,20,25,50,75,100,150,200,250,300,350], label='Nc')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DHiBAqorNmpS"},"source":["train_dataset.to_csv(\"Train.csv\")\n","validation_dataset.to_csv(\"Vali.csv\")\n","test_dataset.to_csv(\"Test.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q62S40oPDMSg"},"source":[""],"execution_count":null,"outputs":[]}]}