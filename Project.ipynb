{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Percolation Thesis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# from numba import cuda\n",
    "\n",
    "# Make numpy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "print(\"Tensorflow\", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(name='HybridEllipsePercolation.txt', sep1=\" \", header1=None, shuffle=True):\n",
    "    data = pd.read_csv(name, sep=sep1, header=header1)\n",
    "    data.columns = [\"r1\", \"2a2\", \"r2\", \"frac\", \"Nc\", \"Nc Std. Dev\", \"eta c\" ]\n",
    "    # data.reset_index(inplace=True)\n",
    "    \n",
    "    if shuffle:\n",
    "        data = data.sample(frac=1).reset_index(drop=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = import_data(shuffle=True)\n",
    "dataset = rawdata.copy()\n",
    "dataset.pop(\"Nc Std. Dev\")\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "dataset.isna().sum()\n",
    "# drop missing values\n",
    "dataset = dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and test sets\n",
    "\n",
    "train_dataset = dataset.sample(frac=1.0, random_state=0)\n",
    "test_dataset = dataset.drop(train_dataset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train_dataset[['r1', '2a2', 'r2', 'frac', 'Nc', 'eta c']], diag_kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.dtypes"
   ]
  },
  {
   "source": [
    "train_features = train_dataset.copy()\n",
    "test_features = test_dataset.copy()\n",
    "\n",
    "train_labels = train_features.pop('eta c')\n",
    "test_labeles = test_features.pop('eta c')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (adaptively) normalize the data using keras \n",
    "\n",
    "normalizer = preprocessing.Normalization()\n",
    "normalizer.adapt(np.array(train_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = np.array(train_features[:1])\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "  print('First example:', first)\n",
    "  print()\n",
    "  print('Normalized:', normalizer(first).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_train_features = normalizer(np.array(train_features.copy())).numpy()"
   ]
  },
  {
   "source": [
    "## Design an MLP for the normalized Data\n",
    "- 5 inputs, 1 output\n",
    "- inputs are -> normalized_train_features\n",
    "- momentum = 0.1\n",
    "- random initial weights\n",
    "- 2 layers\n",
    "- dynamically change the number of neurons in each layer, starting with 5-5-3-1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Tensorflow MLP Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bias(data):\n",
    "    N1 = np.shape(data)[0]\n",
    "    N2 = np.shape(data)[1]\n",
    "    a = -1*np.ones((N1,N2+1))\n",
    "    a[:,:-1] = data\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X = add_bias(normalized_train_features)\n",
    "Y = train_labels\n",
    "# Set the input shape\n",
    "input_shape = (6,)\n",
    "print(f'Feature shape: {input_shape}')\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_shape=input_shape, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Configure the model and start training\n",
    "model.compile(loss='mean_absolute_error', optimizer='SGD', metrics=['mean_squared_error'])\n",
    "model.fit(X, Y, epochs=100, batch_size=200, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "source": [
    "### My Own MLP"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# from numba import cuda\n",
    "\n",
    "class MLP_Regression:\n",
    "    def __init__(self, features, labels, training=True):\n",
    "        \"\"\" Initialises MLP Regression Model. \n",
    "            \"\"\"\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    \n",
    "    def set_architecture(self, number_of_hidden_layers, nodes_in_hidden_layers=[5,3]):\n",
    "        \"\"\" Default is TRAINING with 5-5-3-1 architecture.\n",
    "            Number of hidden layers and length of nodes list must match. \"\"\"\n",
    "            \n",
    "        self.no_hid_layers = number_of_hidden_layers\n",
    "        self.nodes_per_layer = nodes_in_hidden_layers\n",
    "\n",
    "        if len(nodes_in_hidden_layers) != number_of_hidden_layers:\n",
    "            print(\"Number of layers and nodes in hidden layers do not match.\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " if len([5,3,3]) != 2:\n",
    "     print(\"here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}