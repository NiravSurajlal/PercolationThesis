{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "937a9bf7f4d02271b0e84c824f2cfec9d499b9b3c4d4ea54ebae47f399b1cd48"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Preprocessing\n",
    "This notebook attempts to make more sense of the data. The outcome from this notebook will be used to choose machine learning models and data augmentation methods. It will also help understanding the problem.\n",
    "\n",
    "## Assumptions\n",
    "- The long ellipse if given by r1 while the short ellips is given by a2 and r2\n",
    "- 2a1 = 1, thus a1=0.5 and b1=0.5. 2a2<=1.   \n",
    "- The hetrogenous ellipses are split with f stating how many are short. 1-f gives how many are long.\n",
    "- The ellipse ratio r, is given by the major and minor axis lengths such that r=a/b.\n",
    "- The width of the ellipse in a given axis direction is given by 2a or 2b."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "source": [
    "Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(name='HybridEllipsePercolation.txt', sep1=\" \", header1=None, shuffle=True):\n",
    "    data = pd.read_csv(name, sep=sep1, header=header1)\n",
    "    data.columns = [\"r1\", \"2a2\", \"r2\", \"frac\", \"Nc\", \"Nc Std. Dev\", \"eta c\" ]\n",
    "    # data.reset_index(inplace=True)\n",
    "    \n",
    "    if shuffle:\n",
    "        data = data.sample(frac=1).reset_index(drop=True)\n",
    "    return data\n",
    "\n",
    "def single_input_vs_output(dataset, input_column, output_column=\"eta c\", plot=False, output=False):\n",
    "    reduced_dataset = dataset.drop_duplicates(subset=input_column)\n",
    "    # data_range = reduced_dataset[input_column].to_numpy()\n",
    "    if output:\n",
    "        print(\"{col} range is: {rng}    (output is {out})\".format(col=input_column, rng=len(reduced_dataset), out=output_column))\n",
    "\n",
    "    if plot:\n",
    "        ax1 = reduced_dataset.plot.scatter( x=input_column,\n",
    "                        y=output_column,\n",
    "                        c='DarkBlue')\n",
    "        return ax1\n",
    "        # return sns.scatterplot(data=reduced_dataset, x=input_column, y=output_column)\n",
    "    \n",
    "    return reduced_dataset[[input_column, output_column]]\n",
    "\n",
    "def split_data(dataset):\n",
    "    train_dataset = dataset.sample(frac=0.6, random_state=0)\n",
    "    valid_and_test_dataset = dataset.drop(train_dataset.index)\n",
    "    test_dataset = valid_and_test_dataset.sample(frac=0.5, random_state=0)\n",
    "    validation_dataset = valid_and_test_dataset.drop(test_dataset.index)\n",
    "    return train_dataset, test_dataset, validation_dataset\n",
    "\n",
    "def split_features_labels(data, label_column='eta c'):\n",
    "    features = data\n",
    "    labels = data.pop(label_column)\n",
    "    return features, labels"
   ]
  },
  {
   "source": [
    "Importing the data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = import_data(shuffle=False)\n",
    "dataset = rawdata.copy()\n",
    "\n",
    "# remove irrelevant columns\n",
    "# dataset.pop(\"Nc Std. Dev\")\n",
    "\n",
    "# check for missing values\n",
    "dataset.isna().sum()\n",
    "# drop missing values\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe().transpose()"
   ]
  },
  {
   "source": [
    "From the statistics above, we can instantly see that the data is skewed. r1's mean is < 0.5*its ma, and the same goes for 2a2 and r2. we need to look at some distributions of the data. Maybe see some histrograms of frequency as well. If the data is skewed, splitting may not be simple. Some data may have to be left out to prevent biases. It is possible that we can consider this later during tuning. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Here we will extend the dataset to include the major and minor axis lengths "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"a1\"]=1/2\n",
    "dataset[\"b1\"]=dataset[\"a1\"]/dataset[\"r1\"]\n",
    "dataset[\"a2\"]=dataset[\"2a2\"]/2\n",
    "dataset[\"b2\"]=dataset[\"a2\"]/dataset[\"r2\"]\n",
    "dataset.head()"
   ]
  },
  {
   "source": [
    "The next step is to check for repeating values. Printing the returned variable gives all of the values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1_range = single_input_vs_output(dataset, \"r1\")\n",
    "r2_range = single_input_vs_output(dataset, \"r2\")\n",
    "frac_range = single_input_vs_output(dataset, \"frac\")\n",
    "a1_range = single_input_vs_output(dataset, \"a1\")\n",
    "b1_range = single_input_vs_output(dataset, \"b1\")\n",
    "a2_range = single_input_vs_output(dataset, \"a2\")\n",
    "b2_range = single_input_vs_output(dataset, \"b2\")\n",
    "_2a2_range = single_input_vs_output(dataset, \"2a2\")\n",
    "\n",
    "dataset.nunique()"
   ]
  },
  {
   "source": [
    "You can see from theabove that a1 is constant. b2 has the most variation with frac varying the least. Next would be to plot a histogram of the numbers occuring for each one. The histogram plots will be presented as {r1, r2}, {a2,frac}. \n",
    "\n",
    "We have to note the actual numbers for choosing the correct bin size, so viewing the list of present values is useful.\n",
    "\n",
    "r1 -> 1000, r2 -> 1000, frac -> 100, a2 or 2a2 -> 1000"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.distplot(dataset[\"frac\"], kde=False, color='red', bins=100)\n",
    "# plt.title('Frequency of Fraction', fontsize=14)\n",
    "# plt.xlabel('Fraction', fontsize=10)\n",
    "# plt.ylabel('Frequency', fontsize=10)\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(12,6))\n",
    "sns.distplot(dataset[\"r1\"], kde=False, color='red', bins=1000, ax=axs[0])\n",
    "sns.distplot(dataset[\"r2\"], kde=False, color='red', bins=1000, ax=axs[1])"
   ]
  },
  {
   "source": [
    "This is not too visible, so can convert the columns to strings and plot like catagories. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catagorical_dataset = dataset.copy()\n",
    "catagorical_dataset['r1'] = catagorical_dataset['r1'].astype(str)\n",
    "catagorical_dataset['r2'] = catagorical_dataset['r2'].astype(str)\n",
    "sns.displot(catagorical_dataset, x=\"r1\", shrink=.8, color='red', ax=axs[0])\n",
    "sns.displot(catagorical_dataset, x=\"r2\", shrink=.8, color='red', ax=axs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catagorical_dataset['frac'] = catagorical_dataset['frac'].astype(str)\n",
    "sns.displot(catagorical_dataset, x=\"frac\", shrink=.8, color='red', ax=axs[0])\n",
    "\n",
    "catagorical_dataset['a2'] = catagorical_dataset['a2'].astype(str)\n",
    "sns.displot(catagorical_dataset, x=\"a2\", shrink=.8, color='red', ax=axs[1])"
   ]
  },
  {
   "source": [
    "From the plots above, we can see that r1, r2 are relatively consistent in the amount of data for each r value, but the data is skewed towards the smaller r values.\n",
    "\n",
    "frac 0.99 has substantially fewer points than the rest. The frac class is also skewed towards the higher frac numbers, as the intervals decrease after 0.9 (including 0.95 and 0.99).\n",
    "\n",
    "the a2 class (or 2a2) increases by 0.05 between 0.05 and 0.5 (0.1 for 2a2...). However, it also includes {0.005, 0.01, 0.025}, i.e. +0.005, +0.015, +0.025. This means there are more points in the {0-0.05 range than the other intervals}. \n",
    "\n",
    "This can be shown in the distributino plots."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"r1 range {}\".format(list(r1_range[\"r1\"])))\n",
    "print(\"r2 range {}\".format(list(r2_range[\"r2\"])))\n",
    "print(\"frac range {}\".format(list(frac_range[\"frac\"])))\n",
    "print(\"a2 range {}\".format(list(a2_range[\"a2\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, figsize=(12,6))\n",
    "sns.distplot(dataset[\"r1\"], color='red', bins=1000, ax=axs[0])\n",
    "sns.distplot(dataset[\"r2\"], color='red', bins=1000, ax=axs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, figsize=(12,6))\n",
    "sns.distplot(dataset[\"frac\"], color='red', bins=100, ax=axs[0])\n",
    "sns.distplot(dataset[\"a2\"], color='red', bins=100, ax=axs[1])"
   ]
  },
  {
   "source": [
    "Before moving forward, we can look at the output or labels/targets. These are the 'eta c' and 'Nc' columns."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, figsize=(12,6))\n",
    "sns.distplot(dataset[\"eta c\"], color='red', bins=100, ax=axs[0])\n",
    "sns.distplot(dataset[\"Nc\"], color='red', bins=100, ax=axs[1])"
   ]
  },
  {
   "source": [
    "From this we can see that the target distributions are skewed towards the lower end of the specturm. Rescaling 'Nc' between -1 and 1 will result in more dense data with larger outliers. Using 'eta c' as the output may be better. Before emoving on, we should plot all the variables agains one another (see Data.png)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes long to plot -> see saved images.\n",
    "\n",
    "# sns.pairplot(train_dataset[['r1', '2a2', 'r2', 'frac', 'Nc', 'eta c']], diag_kind='kde')"
   ]
  },
  {
   "source": [
    "Pairplot of the data.\n",
    "\n",
    "![Pairplot](Data.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Unfortunatley, the data does not show any direct correlations and a hear map is not very useful here. When looking at the data it is useful to remember that the simulations used the inputs to predict 'eta c' or 'Nc' when percolation in the system will occur. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Now the range of each feature is different and some of them are unbalanced. The features can be standardized between -1 and 1 with a 0 mean. This will be useful to prevent one feature from dominating the weight updates. We can account for the unbalancing by ommiting some data and/or feature selection. However, lets try PCA first and see how the problem looks. \n",
    "\n",
    "NOTE: The targets 'eta c' and 'Nc' have been scaled."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaled_dataset = dataset.copy()\n",
    "scaled_dataset['r1'] = StandardScaler().fit_transform(dataset['r1'].values.reshape(-1,1))\n",
    "scaled_dataset['r2'] = StandardScaler().fit_transform(dataset['r2'].values.reshape(-1,1))\n",
    "scaled_dataset['frac'] = StandardScaler().fit_transform(dataset['frac'].values.reshape(-1,1))\n",
    "scaled_dataset['2a2'] = StandardScaler().fit_transform(dataset['a2'].values.reshape(-1,1))\n",
    "scaled_dataset['eta c'] = StandardScaler().fit_transform(dataset['eta c'].values.reshape(-1,1))\n",
    "scaled_dataset['Nc'] = StandardScaler().fit_transform(dataset['Nc'].values.reshape(-1,1))\n",
    "\n",
    "#account for added features\n",
    "scaled_dataset[\"a1\"]=1/2\n",
    "scaled_dataset[\"b1\"]=scaled_dataset[\"a1\"]/scaled_dataset[\"r1\"]\n",
    "scaled_dataset[\"a2\"]=scaled_dataset[\"2a2\"]/2\n",
    "scaled_dataset[\"b2\"]=scaled_dataset[\"a2\"]/scaled_dataset[\"r2\"]\n",
    "\n",
    "scaled_dataset.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_dataset.head()"
   ]
  },
  {
   "source": [
    "We can visualise the (original) features and targets again to understand the scaling."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(ncols=2, figsize=(12,6))\n",
    "# sns.distplot(scaled_dataset[\"eta c\"], color='red', bins=100, ax=axs[0])\n",
    "# sns.distplot(scaled_dataset[\"Nc\"], color='red', bins=100, ax=axs[1])\n",
    "# fig.savefig(\"scaled_labels.png\")\n",
    "\n",
    "# fig, axs = plt.subplots(ncols=2, figsize=(12,6))\n",
    "# sns.distplot(scaled_dataset[\"r1\"], color='red', bins=1000, ax=axs[0])\n",
    "# sns.distplot(scaled_dataset[\"r2\"], color='red', bins=1000, ax=axs[1])\n",
    "# fig.savefig(\"scaled_r_features.png\")\n",
    "\n",
    "# fig, axs = plt.subplots(ncols=2, figsize=(12,6))\n",
    "# sns.distplot(scaled_dataset[\"frac\"], color='red', bins=100, ax=axs[0])\n",
    "# sns.distplot(scaled_dataset[\"a2\"], color='red', bins=100, ax=axs[1])\n",
    "# fig.savefig(\"scaled_frac_a2.png\")"
   ]
  },
  {
   "source": [
    "The scaled data.\n",
    "\n",
    "![scaled_r](scaled_r_features.png)\n",
    "![scaled_frac](scaled_frac_a2.png)\n",
    "![scaled_labels](scaled_labels.png)\n",
    "\n",
    "Here you can clearly see that r1 and r2 have a bias with large outliers. frac and a2 have pretty uniform distributions and can be handled pretty easily by ommiting samples at the lower and higher end. For now I do not think this is necessary because the difference in the number of samples is not huge and will effect the other distributions. eta c still looks like a better label, with a smaller range, fewer outliers and less of a bias than Nc. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We can look at the covariance matrix of the data, but we know that they were slected relatively independantley. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_dataset[['r1','r2','2a2','frac']].cov()"
   ]
  },
  {
   "source": [
    "Now we will implement PCA to try and reduce the number of input features. The total number of un expanded features is 4. With the expansion, we get 8. First we will try PCA on the 4, then the 8 and then maybe a selection of the features. For now the labels will be the nonstandardizedd 'eta c'. In the future we can possibly use both 'eta c' and 'Nc' or their scaled versions. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "features = scaled_dataset.drop(['eta c', 'Nc', 'Nc Std. Dev', 'a1', 'b1', 'a2', 'b2'], axis=1)\n",
    "expanded_features = scaled_dataset.drop(['eta c', 'Nc', 'Nc Std. Dev',], axis=1)\n",
    "labels = scaled_dataset['eta c']\n",
    "\n",
    "components = 2\n",
    "components_cols = ['PC 1', 'PC 2']\n",
    "X = features\n",
    "Y = labels\n",
    "\n",
    "pca = PCA(n_components=components)\n",
    "pca_data = pca.fit_transform(X.values)\n",
    "\n",
    "principalDf = pd.DataFrame(data = pca_data, columns = components_cols)\n",
    "\n",
    "pcaDF = pd.concat([principalDf, Y], axis = 1)\n",
    "pcaDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(ncols=1, figsize=(12,6))\n",
    "# sns.distplot(scaled_dataset[\"frac\"], color='red', bins=100, ax=axs[0])\n",
    "# sns.distplot(scaled_dataset[\"a2\"], color='red', bins=100, ax=axs[1])\n",
    "fig = sns.pairplot(pcaDF[['PC 1', 'PC 2', 'eta c']], diag_kind='kde')\n",
    "fig.savefig(\"PCA.png\")\n"
   ]
  },
  {
   "source": [
    "The PCA data has been plotted in a pair plot to see how the new components relate to the output. \n",
    "\n",
    "![pca](PCA.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "An attempt with 1 component."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = 1\n",
    "components_cols = ['PC 1']\n",
    "X = features\n",
    "Y = labels\n",
    "\n",
    "pca = PCA(n_components=components)\n",
    "pca_data = pca.fit_transform(X.values)\n",
    "\n",
    "principalDf = pd.DataFrame(data = pca_data, columns = components_cols)\n",
    "\n",
    "pcaDF = pd.concat([principalDf, Y], axis = 1)\n",
    "pcaDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.pairplot(pcaDF[['PC 1', 'eta c']], diag_kind='kde')\n",
    "fig.savefig(\"OnePCA.png\")"
   ]
  },
  {
   "source": [
    "Lastly, we will store the dataframes to csv files to be used in other notebooks."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_dataset.to_csv(\"NormalizedData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaDF.to_csv(\"PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}