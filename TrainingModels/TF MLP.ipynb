{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TF MLP.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1IjZARY1zynmfu7wKFsceoRZrRpQBku1L","authorship_tag":"ABX9TyPviBvQMzvlXv124w3AhvUk"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"OHLSa2Mq7TwB"},"source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MaxAbsScaler\n","import seaborn as sns\n","sns.set()\n","\n","seed_val = 2021\n","np.random.seed(seed_val)\n","\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers.experimental import preprocessing\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.regularizers import l1, l2, l1_l2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"parLavZ2HMZe"},"source":["def check_representation(train_dataset, validation_dataset, test_dataset, label_name):\n","  # sort the data to check representation\n","  sorted_train = train_dataset.sort_index()\n","  sorted_test = test_dataset.sort_index()\n","\n","  # sort the data to check representation\n","  sorted_train = train_dataset.sort_index()\n","  sorted_vali = validation_dataset.sort_index()\n","  sorted_test = test_dataset.sort_index()\n","\n","  # check representation\n","  fig = plt.figure()\n","  fig, ax = plt.subplots(ncols=3, nrows=2, figsize=(15,12))\n","\n","  ax[0][0].scatter(x=np.arange(len(sorted_train)), y=sorted_train[label_name], marker='.', alpha=0.4)\n","  ax[0][0].set_title(\"Index Sorted Training Data\")\n","  ax[0][1].scatter(x=np.arange(len(sorted_vali)), y=sorted_vali[label_name], marker='.', alpha=0.4)\n","  ax[0][1].set_title(\"Index Sorted Validation Data\")\n","  ax[0][2].scatter(x=np.arange(len(sorted_test)), y=sorted_test[label_name], marker='.', alpha=0.4)\n","  ax[0][2].set_title(\"Index Sorted Testing Data\")\n","  ax[0][0].set_ylabel(label_name)\n","\n","  train1 = sorted_train.sort_values(label_name)\n","  test1 = sorted_test.sort_values(label_name)\n","\n","  y_val = train1[int(len(train1)/2)-1:int(len(train1)/2)][label_name]\n","  ax[1][0].scatter(x=np.arange(len(train_dataset)), y=train1[label_name], marker='.', alpha=0.4)\n","  # vertical line\n","  ax[1][0].plot([len(train_dataset)/2, len(train_dataset)/2], [0,train_dataset[label_name].max()], '--', alpha=0.4, color=\"red\")\n","  # horizontal line\n","  ax[1][0].plot([0, len(train_dataset)], [y_val,y_val], '--',  alpha=0.4, color=\"red\")\n","  ax[1][0].set_title(\"Label Sorted Training Data\")\n","\n","  sorted_ds=sorted_vali.sort_values(label_name)\n","  y_val = sorted_ds[int(len(sorted_ds)/2)-1:int(len(sorted_ds)/2)][label_name]\n","  ax[1][1].scatter(x=np.arange(len(sorted_ds)), y=sorted_ds[label_name], marker='.', alpha=0.4)\n","  # vertical line\n","  ax[1][1].plot([len(sorted_ds)/2, len(sorted_ds)/2], [0,sorted_ds[label_name].max()], '--',  alpha=0.4, color=\"red\")\n","  # horizontal line\n","  ax[1][1].plot([0, len(sorted_ds)], [y_val,y_val], '--',  alpha=0.4, color=\"red\")\n","  ax[1][1].set_title(\"Label Sorted Train Data\")\n","\n","  sorted_ds=sorted_test.sort_values(label_name)\n","  y_val = sorted_ds[int(len(sorted_ds)/2)-1:int(len(sorted_ds)/2)][label_name]\n","  ax[1][2].scatter(x=np.arange(len(sorted_ds)), y=sorted_ds[label_name], marker='.', alpha=0.4)\n","  # vertical line\n","  ax[1][2].plot([len(sorted_ds)/2, len(sorted_ds)/2], [0,sorted_ds[label_name].max()], '--',  alpha=0.4, color=\"red\")\n","  # horizontal line\n","  ax[1][2].plot([0, len(sorted_ds)], [y_val,y_val], '--',  alpha=0.4, color=\"red\")\n","  ax[1][2].set_title(\"Label Sorted Test Data\")\n","  ax[1][0].set_ylabel(label_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tFvc_fSL7uB0"},"source":["train_dataset = pd.read_csv(\"Train.csv\")\n","validation_dataset = pd.read_csv(\"Vali.csv\")\n","test_dataset = pd.read_csv(\"Test.csv\")\n","  \n","train_dataset.pop(\"Unnamed: 0\")\n","validation_dataset.pop(\"Unnamed: 0\")\n","test_dataset.pop(\"Unnamed: 0\")\n","\n","label_name = 'Nc'\n","in_features = 5\n","out_nodes = 1\n","\n","check_representation(train_dataset, validation_dataset, test_dataset, label_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DHiBAqorNmpS"},"source":["sorted_train = train_dataset.sort_values(label_name)\n","sorted_vali = validation_dataset.sort_values(label_name)\n","sorted_test = test_dataset.sort_values(label_name)\n","\n","# Generate the inputs and labels\n","\n","# TRAIN DATA\n","train = sorted_train.copy()\n","train_features = train.to_numpy()[:,0:in_features]\n","train_labels = train.to_numpy()[:,in_features]\n","X_Sorted = train_features.copy()\n","Y_Sorted = train_labels.copy()\n","# train = sorted_train\n","train = sorted_train.sample(frac=1, random_state=seed_val)\n","train_features = train.to_numpy()[:,0:in_features]\n","train_labels = train.to_numpy()[:,in_features]\n","X = train_features.copy()\n","Y = train_labels.copy()\n","\n","\n","# TRAINING VALIDATION DATA\n","vali = sorted_vali\n","# vali = sorted_vali.sample(frac=1, random_state=seed_val)\n","vali_features = vali.to_numpy()[:,0:in_features]\n","vali_labels = vali.to_numpy()[:,in_features]\n","X_vali = vali_features.copy()\n","Y_vali = vali_labels.copy()\n","\n","# TEST DATA\n","test = sorted_test\n","# test = sorted_test.sample(frac=1, random_state=seed_val)\n","test_features = test.sort_values(label_name).to_numpy()[:,0:in_features]\n","test_labels = test.sort_values(label_name).to_numpy()[:,in_features]\n","X_test = test_features.copy()\n","Y_test = test_labels.copy()\n","\n","eval_set = [(X,Y), (X_vali, Y_vali)]\n","eval_set = [(X_test, Y_test), (X,Y), (X_vali, Y_vali)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oJ4cyM5KkLs5"},"source":["## MLP Class\n","This was easier to use at some stage, and the model was trained using it. However, it is not necessary to put the TF model in a class."]},{"cell_type":"code","metadata":{"id":"9lOHKD158Iks"},"source":["class MLP_Regression():\n","    def __init__(self, input_shape, nodes=30, dropout=False, dropoutrate=0.2, seed=97, kernel_reg=None, use_bias=True, activation=\"relu\", kernel_initializer='he_normal'):\n","        self.model = Sequential()\n","        self.model.add(Dense(nodes, input_shape=(input_shape,), activation=activation, use_bias=use_bias, kernel_regularizer=kernel_reg, kernel_initializer=kernel_initializer))\n","        if dropout:\n","            self.add_dropout(dropoutrate, seed)\n","\n","    def learning_rate(self, initial_lr=1e-2, decay_steps=1e5, decay_rate=0.9):\n","        self.lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=initial_lr, decay_steps=decay_steps, decay_rate=decay_rate)\n","\n","    def SGDoptimizer(self, momentum=0.1, nesterov=False, initial_lr=1e-2, decay_steps=1e5, decay_rate=0.9):\n","        self.learning_rate(initial_lr, decay_steps, decay_rate)\n","        self.optimizer = keras.optimizers.SGD(learning_rate=self.lr_schedule, momentum=momentum, nesterov=nesterov)\n","\n","    def ADAMoptimizer(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False):\n","        self.optimizer = keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, amsgrad=amsgrad, name=\"Adam\")\n","    \n","    def NADAMoptimizer(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07):\n","        self.optimizer = keras.optimizers.Nadam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, name=\"Nadam\")\n","\n","    def add_layer(self, num_nodes, kernel_reg=None, use_bias=True, activation=\"relu\", kernel_initializer='he_normal'):\n","        self.model.add(Dense(num_nodes, activation=activation, use_bias=use_bias, kernel_regularizer=kernel_reg, kernel_initializer=kernel_initializer))\n","    \n","    def add_output_layer(self, out_nodes, kernel_reg=None, use_bias=True, activation=\"relu\", kernel_initializer='he_normal'):\n","        self.model.add(Dense(out_nodes, activation=activation, use_bias=use_bias, kernel_regularizer=kernel_reg, kernel_initializer=kernel_initializer))\n","\n","    def add_dropout(self, rate, seed):\n","        self.model.add(keras.layers.Dropout(rate=rate, seed=seed))\n","\n","    def compileModel(self, optimizer, loss=\"mean_squared_error\", metrics=[\"MSE\",\"MAE\", \"MAPE\"]):\n","        self.model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n","\n","    def train_model(self, X, Y, early_stopping=True, patience=3, epochs=10, batch_size=500, vali_split=0.2, shuffle=False, loss_monitor=\"loss\"):\n","        if early_stopping:\n","            earlystop_callback = EarlyStopping(monitor=loss_monitor, min_delta=0, patience=patience, mode=\"min\", restore_best_weights=True)\n","            hist = self.model.fit(X, Y, epochs=epochs, batch_size=batch_size, validation_split=vali_split, callbacks=[earlystop_callback], shuffle=shuffle)\n","            self.history = pd.DataFrame(hist.history)\n","            self.history['epoch'] = hist.epoch\n","        else:\n","            hist = self.model.fit(X, Y, epochs=epochs, batch_size=batch_size, validation_split=vali_split, shuffle=shuffle)\n","            self.history = pd.DataFrame(hist.history)\n","            self.history['epoch'] = hist.epoch\n","\n","    def show_training_errors(self, save_img=False, name=None):\n","        hist = self.history\n","\n","        fig1 = plt.figure()\n","        fig1, ax = plt.subplots(nrows=2, ncols=2, figsize=(15,12))\n","\n","        ax[0][0].set_xlabel('Epoch')\n","        ax[0][0].set_ylabel('MSE')\n","        ax[0][0].plot(hist['epoch'], hist['MSE'], label='Train Error')\n","        ax[0][0].plot(hist['epoch'], hist['val_MSE'], label='Val Error')\n","        ax[0][0].legend()\n","        ax[0][0].set_title(\"MSE Error\")\n","\n","        ax[0][1].set_xlabel('Epoch')\n","        ax[0][1].set_ylabel('MAE')\n","        ax[0][1].plot(hist['epoch'], hist['MAE'], label='Train Error')\n","        ax[0][1].plot(hist['epoch'], hist['val_MAE'], label='Val Error')\n","        ax[0][1].legend()\n","        ax[0][1].set_title(\"MAE Error\")\n","\n","        ax[1][0].set_xlabel('Epoch')\n","        ax[1][0].set_ylabel('MAPE')\n","        ax[1][0].plot(hist['epoch'], hist['MAPE'], label='Train Mean Abs {} Error'.format(\"%\"))\n","        ax[1][0].plot(hist['epoch'], hist['val_MAPE'], label='Val Mean Abs {} Error'.format(\"%\"))\n","        ax[1][0].legend()\n","        ax[1][0].set_title(\"MAPE\")\n","        if save_img:\n","          fig1.savefig(\"{}_E.png\".format(name))\n","\n","\n","    def test(self, test_features, test_labels, save_img=False, name=None, label_name=\"Nc\"):\n","        test_input = test_features\n","        test_output = np.reshape(test_labels, (-1,1))\n","        prediction = self.model.predict(test_input)\n","        error = test_output-prediction\n","        acc = (abs(error)/test_output)\n","        \n","\n","        fig2 = plt.figure()\n","        fig2, ax = plt.subplots(nrows=3, figsize=(15,16))\n","        # ax[0].plot(np.arange(len(test_output)), test_output, label=\"TestData\")\n","        # ax[0].plot(np.arange(len(prediction)), prediction, label=\"Prediction\", alpha=0.4)\n","        ax[0].scatter(np.arange(len(prediction)), prediction, label=\"Prediction\", alpha=0.4, marker='.')\n","        ax[0].scatter(np.arange(len(test_output)), test_output, label=\"TestData\", marker='.')\n","        ax[0].legend()\n","        ax[0].set_title(\"Model Prediction\")\n","        ax[0].set_ylabel(label_name)\n","\n","        ax[1].scatter(np.arange(len(error)), error, label=\"test_label-pred\", marker='.')\n","        ax[1].legend()\n","        ax[1].set_ylabel(\"Error\")\n","        # ax[1].set_title(\"Model Prediction\")\n","\n","        ax[2].scatter(np.arange(len(acc)), acc*100, label=\"|(error/test_label)|*100\", marker='.')\n","        ax[2].legend()\n","        ax[2].set_ylabel(\"PE\")\n","        if save_img:\n","          fig2.savefig(\"{}_P.png\".format(name))\n","    \n","    def retrainable_save(self, loc_and_name):\n","        self.model.save(\"{}\".format(loc_and_name))    \n","        self.history.to_csv(\"{}.csv\".format(loc_and_name))\n","\n","    def results(self, test_features, test_labels, saveit=False, name=None):\n","        my_model.show_training_errors(save_img=saveit, name=name)\n","        my_model.test(test_features, test_labels, save_img=saveit, name=name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qITQcz8z8vey"},"source":["# reg = l1(0.02)\n","# reg = l1_l2(l1=0.02, l2=0.02)\n","# reg = l2(1e-5)\n","reg = None\n","\n","# Create the Model\n","my_model = MLP_Regression(in_features, nodes=100, dropout=False, dropoutrate=0.1, kernel_reg=reg, use_bias=True, activation=\"relu\")\n","\n","# select optimizer\n","# my_model.SGDoptimizer(momentum=0.6, nesterov=True, initial_lr=1e-2, decay_steps=1e5, decay_rate=0.9)\n","# my_model.ADAMoptimizer(learning_rate=1e-3, beta_1=0.97)\n","my_model.NADAMoptimizer(learning_rate=8e-4, beta_1=0.99, beta_2=0.999)\n","\n","# add layers\n","my_model.add_layer(50, kernel_reg=reg, use_bias=True, activation=\"sigmoid\")\n","my_model.add_layer(25, kernel_reg=reg, use_bias=True, activation=\"sigmoid\")\n","my_model.add_layer(13, kernel_reg=reg, use_bias=True, activation=\"relu\")\n","# add output layer\n","my_model.add_output_layer(out_nodes, kernel_reg=reg, use_bias=True, activation=\"linear\")\n","\n","# compile and define loss metrics\n","my_model.compileModel(my_model.optimizer, loss=\"mean_absolute_percentage_error\")\n","\n","# The model\n","my_model.model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qk6qUDGmW4PS"},"source":["#_______________________________________________________________________________________________________________________\n","# Train the model\n","if len(train_dataset) > len(dataset):\n","  print(\"Noisey Training\")\n","else:\n","  print(\"Non-Noisey Training\")\n","print()\n","\n","print(\"Training has started on {} samples.\".format(len(train_features)))\n","\n","my_model.train_model(X,Y, epochs=200, patience=15, batch_size=8, shuffle=True, early_stopping=True, loss_monitor=\"val_loss\", vali_split=0.1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R36MK7QCHXVG"},"source":["## Get Results\n","Run the 2nd cell after this to save the model. The results save 'X_E' for the erros and 'X_P' for the predictions where X is defined by the user. Can change this in the class as is necessary."]},{"cell_type":"code","metadata":{"id":"K4Sqto2IMh2C"},"source":["_NAME = \"NNModel4\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"olv2rEk6dson"},"source":["#_______________________________________________________________________________________________________________________\n","# Save the model\n","my_model.retrainable_save(loc_and_name=_NAME)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TnHtLZBqMora"},"source":["#_______________________________________________________________________________________________________________________\n","# Save the model\n","_NAME1 = _NAME+\"Train\"\n","\n","my_model.results(X_Sorted, Y_Sorted, saveit=True, name=_NAME1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1A9OFvDqMohO"},"source":["#_______________________________________________________________________________________________________________________\n","# Save the model\n","_NAME2 = _NAME+\"Vali\"\n","\n","my_model.results(X_vali, Y_vali, saveit=True, name=_NAME2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WN1j9GUKOByC"},"source":["#_______________________________________________________________________________________________________________________\n","# Save the model\n","_NAME3 = _NAME+\"Test\"\n","\n","my_model.results(X_test, Y_test, saveit=True, name=_NAME3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_CJAetf48kCd"},"source":["my_model.model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KZxlHcPUlpJp"},"source":["## Second Round of Training?\n","This can be used the same as the first, but using the weights from the previous model as a starting point. Not recommended."]},{"cell_type":"markdown","metadata":{"id":"SCP3oJy5jCkT"},"source":["# Loaded Model Predictor"]},{"cell_type":"code","metadata":{"id":"wwJfmltDjErT"},"source":["class LoadedMLP_Regression():\n","    def __init__(self, model):\n","        self.model = model\n","\n","    def learning_rate(self, initial_lr=1e-2, decay_steps=1e5, decay_rate=0.9):\n","        self.lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=initial_lr, decay_steps=decay_steps, decay_rate=decay_rate)\n","\n","    def SGDoptimizer(self, momentum=0.1, nesterov=False, initial_lr=1e-2, decay_steps=1e5, decay_rate=0.9):\n","        self.learning_rate(initial_lr, decay_steps, decay_rate)\n","        self.optimizer = keras.optimizers.SGD(learning_rate=self.lr_schedule, momentum=momentum, nesterov=nesterov)\n","\n","    def ADAMoptimizer(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False):\n","        self.optimizer = keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, amsgrad=amsgrad, name=\"Adam\")\n","    \n","    def NADAMoptimizer(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07):\n","        self.optimizer = keras.optimizers.Nadam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, name=\"Nadam\")\n","\n","    def add_layer(self, num_nodes, kernel_reg=None, use_bias=True, activation=\"relu\", kernel_initializer='he_normal'):\n","        self.model.add(Dense(num_nodes, activation=activation, use_bias=use_bias, kernel_regularizer=kernel_reg, kernel_initializer=kernel_initializer))\n","    \n","    def add_output_layer(self, out_nodes, kernel_reg=None, use_bias=True, activation=\"relu\", kernel_initializer='he_normal'):\n","        self.model.add(Dense(out_nodes, activation=activation, use_bias=use_bias, kernel_regularizer=kernel_reg, kernel_initializer=kernel_initializer))\n","\n","    def add_dropout(self, rate, seed):\n","        self.model.add(keras.layers.Dropout(rate=rate, seed=seed))\n","\n","    def compileModel(self, optimizer, loss=\"mean_squared_error\", metrics=[\"MSE\",\"MAE\", \"MAPE\"]):\n","        self.model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n","\n","    def train_model(self, X, Y, early_stopping=True, patience=3, epochs=10, batch_size=500, vali_split=0.2, shuffle=False, loss_monitor=\"loss\"):\n","        if early_stopping:\n","            earlystop_callback = EarlyStopping(monitor=loss_monitor, min_delta=0, patience=patience, mode=\"min\", restore_best_weights=True)\n","            hist = self.model.fit(X, Y, epochs=epochs, batch_size=batch_size, validation_split=vali_split, callbacks=[earlystop_callback], shuffle=shuffle)\n","            self.history = pd.DataFrame(hist.history)\n","            self.history['epoch'] = hist.epoch\n","        else:\n","            hist = self.model.fit(X, Y, epochs=epochs, batch_size=batch_size, validation_split=vali_split, shuffle=shuffle)\n","            self.history = pd.DataFrame(hist.history)\n","            self.history['epoch'] = hist.epoch\n","\n","    def show_training_errors(self, save_img=False, name=None):\n","        hist = self.history\n","\n","        fig1 = plt.figure()\n","        fig1, ax = plt.subplots(nrows=2, ncols=2, figsize=(15,12))\n","\n","        ax[0][0].set_xlabel('Epoch')\n","        ax[0][0].set_ylabel('MSE')\n","        ax[0][0].plot(hist['epoch'], hist['MSE'], label='Train Error')\n","        ax[0][0].plot(hist['epoch'], hist['val_MSE'], label='Val Error')\n","        ax[0][0].legend()\n","        ax[0][0].set_title(\"MSE Error\")\n","\n","        ax[0][1].set_xlabel('Epoch')\n","        ax[0][1].set_ylabel('MAE')\n","        ax[0][1].plot(hist['epoch'], hist['MAE'], label='Train Error')\n","        ax[0][1].plot(hist['epoch'], hist['val_MAE'], label='Val Error')\n","        ax[0][1].legend()\n","        ax[0][1].set_title(\"MAE Error\")\n","\n","        ax[1][0].set_xlabel('Epoch')\n","        ax[1][0].set_ylabel('MAPE')\n","        ax[1][0].plot(hist['epoch'], hist['MAPE'], label='Train Mean Abs {} Error'.format(\"%\"))\n","        ax[1][0].plot(hist['epoch'], hist['val_MAPE'], label='Val Mean Abs {} Error'.format(\"%\"))\n","        ax[1][0].legend()\n","        ax[1][0].set_title(\"MAPE\")\n","        if save_img:\n","          fig1.savefig(\"{}_E.png\".format(name))\n","\n","\n","    def test(self, test_features, test_labels, save_img=False, name=None, label_name=\"Nc\"):\n","        test_input = test_features\n","        test_output = np.reshape(test_labels, (-1,1))\n","        prediction = self.model.predict(test_input)\n","        error = test_output-prediction\n","        acc = (abs(error)/test_output)\n","        \n","\n","        fig2 = plt.figure()\n","        fig2, ax = plt.subplots(nrows=3, figsize=(15,16))\n","        # ax[0].plot(np.arange(len(test_output)), test_output, label=\"TestData\")\n","        # ax[0].plot(np.arange(len(prediction)), prediction, label=\"Prediction\", alpha=0.4)\n","        ax[0].scatter(np.arange(len(prediction)), prediction, label=\"Prediction\", alpha=0.4, marker='.')\n","        ax[0].scatter(np.arange(len(test_output)), test_output, label=\"TestData\", marker='.')\n","        ax[0].legend()\n","        ax[0].set_title(\"Model Prediction\")\n","        ax[0].set_ylabel(label_name)\n","\n","        ax[1].scatter(np.arange(len(error)), error, label=\"test_label-pred\", marker='.')\n","        ax[1].legend()\n","        ax[1].set_ylabel(\"Error\")\n","        # ax[1].set_title(\"Model Prediction\")\n","\n","        ax[2].scatter(np.arange(len(acc)), acc*100, label=\"|(error/test_label)|*100\", marker='.')\n","        ax[2].legend()\n","        ax[2].set_ylabel(\"PE\")\n","        if save_img:\n","          fig2.savefig(\"{}_P.png\".format(name))\n","    \n","    def retrainable_save(self, loc_and_name):\n","        self.model.save(\"{}\".format(loc_and_name))    \n","        self.history.to_csv(\"{}.csv\".format(loc_and_name))\n","\n","    def results(self, test_features, test_labels, saveit=False, name=None):\n","        self.show_training_errors(save_img=saveit, name=name)\n","        self.test(test_features, test_labels, save_img=saveit, name=name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BGArCT-JCBUX"},"source":["train_dataset_old = train_dataset.copy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wGug61IYmK0E"},"source":["load = keras.models.load_model(\"/\"+_NAME)\n","# load model\n","loaded_model = LoadedMLP_Regression(load)\n","\n","# choose optimizer\n","# loaded_model.SGDoptimizer(momentum=0.3, nesterov=True, initial_lr=1e-7, decay_steps=1e5, decay_rate=0.95)\n","# loaded_model.ADAMoptimizer(learning_rate=1e-5, beta_1=0.97)\n","loaded_model.NADAMoptimizer(learning_rate=1e-6, beta_1=0.99)\n","\n","# compile and define loss metrics\n","loaded_model.compileModel(loaded_model.optimizer, loss=\"mean_absolute_percentage_error\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UeXc6WlynrZW"},"source":["loaded_model.model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SMYz1_fByk9G"},"source":["\n","#_______________________________________________________________________________________________________________________\n","# Train the model\n","if len(train_dataset) > len(dataset):\n","  print(\"Noisey Training\")\n","else:\n","  print(\"Non-Noisey Training\")\n","print()\n","\n","print(\"Re-Training has started on {} samples.\".format(len(train_features)))\n","\n","loaded_model.train_model(X,Y, epochs=400, patience=20, batch_size=8, shuffle=True, early_stopping=True, loss_monitor=\"val_loss\", vali_split=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"irCoa_DNmcbA"},"source":["## Get Results\n","Basically the same as before, except with added suffixes."]},{"cell_type":"code","metadata":{"id":"o8y30QVjaisH"},"source":["#_______________________________________________________________________________________________________________________\n","# Save the model\n","_NAME_2 = _NAME+\"Retrain\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sQittaCZmZ5S"},"source":["#_______________________________________________________________________________________________________________________\n","# Save the model\n","my_model.retrainable_save(loc_and_name=_NAME_2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yQa5NcjRL613"},"source":["my_model.retrainable_save(loc_and_name=_NAME_2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PLowm-XYLaMn"},"source":["_NAME3 = _NAME_2+\"Tr\"\n","\n","my_model.results(X.copy(), Y.copy(), saveit=True, name=_NAME3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JrJfdBqiLdih"},"source":["#_______________________________________________________________________________________________________________________\n","# Get the results on the test or training data\n","# Use TEST DATA\n","_NAME3 = _NAME_2+\"V\"\n","\n","my_model.results(X_vali, Y_vali, saveit=True, name=_NAME3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LuCc8iXSLdYf"},"source":["#_______________________________________________________________________________________________________________________\n","# Get the results on the test or training data\n","# Use TEST DATA\n","_NAME3 = _NAME_2+\"Te\"\n","\n","my_model.results(X_test.copy(), Y_test.copy(), saveit=True, name=_NAME3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q62S40oPDMSg"},"source":[""],"execution_count":null,"outputs":[]}]}