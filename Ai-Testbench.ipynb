{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "937a9bf7f4d02271b0e84c824f2cfec9d499b9b3c4d4ea54ebae47f399b1cd48"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Test Different MLP\n",
    "Here we will test differet versions. Maybe running the model multiple times and averaging the results. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "seed_val = 97\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset, seed, train_ratio=0.6, shuffle=True):\n",
    "    if shuffle:\n",
    "        dataset = dataset.sample(frac=1, random_state=seed)#.reset_index(drop=True)\n",
    "\n",
    "    train_dataset = dataset.sample(frac=train_ratio, random_state=0)\n",
    "    test_dataset = dataset.drop(train_dataset.index)\n",
    "    \n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def add_bias(data):\n",
    "    N1 = np.shape(data)[0]\n",
    "    N2 = np.shape(data)[1]\n",
    "    a = -1*np.ones((N1,N2+1))\n",
    "    a[:,:-1] = data\n",
    "    return a\n",
    "\n",
    "def add_noise(dataset, target_column=4, noise_var=0.01, input_n=False, output_n=False):\n",
    "    \"\"\" Called on DATAFRAME training data. \"\"\"\n",
    "    features = dataset.to_numpy()[:,0:target_column]\n",
    "    labels = np.reshape(dataset.to_numpy()[:,target_column], (-1,1))\n",
    "    \n",
    "    if input_n:\n",
    "        noise = np.reshape(np.random.normal(0,0.01,np.shape(features)[0]*np.shape(features)[1]),(np.shape(features)[0],np.shape(features)[1]))\n",
    "        features = features + noise \n",
    "\n",
    "    if output_n:\n",
    "        noise = np.reshape(np.random.normal(0,0.01,np.shape(labels)[0]*np.shape(labels)[1]),(np.shape(labels)[0],np.shape(labels)[1]))\n",
    "        labels = labels + noise\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"LIM_scaled.csv\"\n",
    "name = \"data.csv\"\n",
    "\n",
    "dataset = pd.read_csv(name)\n",
    "# data2.describe().transpose()\n",
    "dataset.pop(\"Unnamed: 0\")\n",
    "\n",
    "# scale the data\n",
    "scaled_dataset = dataset.copy()\n",
    "scaled_dataset['b1'] = MaxAbsScaler().fit_transform(dataset['b1'].values.reshape(-1,1))\n",
    "scaled_dataset['a2'] = MaxAbsScaler().fit_transform(dataset['a2'].values.reshape(-1,1))\n",
    "scaled_dataset['RatioTotalArea'] = MaxAbsScaler().fit_transform(dataset['RatioTotalArea'].values.reshape(-1,1))\n",
    "scaled_dataset['frac'] = MaxAbsScaler().fit_transform(dataset['frac'].values.reshape(-1,1))\n",
    "\n",
    "# split the data\n",
    "train_dataset, test_dataset = split_data(scaled_dataset.copy(), seed_val, train_ratio=0.8)\n",
    "\n",
    "# sort the data to check representation\n",
    "sorted_train = train_dataset.sort_index()\n",
    "sorted_test = test_dataset.sort_index()\n",
    "\n",
    "# check representation\n",
    "fig = plt.figure()\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(15,7))\n",
    "\n",
    "ax[0].scatter(x=np.arange(len(sorted_train)), y=sorted_train['eta c'], marker='.', alpha=0.4)\n",
    "ax[1].scatter(x=np.arange(len(sorted_test)), y=sorted_test['eta c'], marker='.', alpha=0.4)\n",
    "ax[0].set_ylabel(\"eta c\")\n",
    "ax[0].set_title(\"Training Data\")\n",
    "ax[1].set_title(\"Testing Data\")\n"
   ]
  },
  {
   "source": [
    "ML Algo class."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Regression():\n",
    "    def __init__(self, input_shape, nodes=30, dropout=False, rate=0.2, seed=97):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(nodes, input_shape=(input_shape,), activation=\"relu\", use_bias=True))\n",
    "        if dropout:\n",
    "            self.add_dropout(rate, seed)\n",
    "\n",
    "    def learning_rate(self, initial_lr=1e-2, decay_steps=1e5, decay_rate=0.9):\n",
    "        self.lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=initial_lr, decay_steps=decay_steps, decay_rate=decay_rate)\n",
    "\n",
    "    def optimizer(self, momentum=0.1):\n",
    "        self.optimizer = keras.optimizers.SGD(learning_rate=self.lr_schedule, momentum=momentum, nesterov=False)\n",
    "\n",
    "    def add_layer(self, num_nodes):\n",
    "        self.model.add(Dense(num_nodes, activation=\"relu\", use_bias=True))\n",
    "    \n",
    "    def add_output_layer(self, out_nodes):\n",
    "        self.model.add(Dense(out_nodes, activation=\"relu\", use_bias=True))\n",
    "\n",
    "    def add_dropout(self, rate, seed):\n",
    "        self.model.add(keras.layers.Dropout(rate=rate, seed=seed))\n",
    "\n",
    "    def compileModel(self, optimizer, loss=\"mean_squared_error\", metrics=[\"MSE\",\"MAE\", \"MAPE\"]):\n",
    "        self.model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "    def train_model(self, X, Y, early_stopping=True, patience=3, epochs=10, batch_size=500, vali_split=0.2):\n",
    "        if early_stopping:\n",
    "            earlystop_callback = EarlyStopping(monitor=\"loss\", min_delta=0, patience=patience, mode=\"min\", restore_best_weights=True)\n",
    "            hist = self.model.fit(X, Y, epochs=epochs, batch_size=batch_size, validation_split=vali_split, callbacks=[earlystop_callback])\n",
    "            self.history = pd.DataFrame(hist.history)\n",
    "            self.history['epoch'] = hist.epoch\n",
    "        else:\n",
    "            hist = self.model.fit(X, Y, epochs=epochs, batch_size=batch_size, validation_split=vali_split)\n",
    "            self.history = pd.DataFrame(hist.history)\n",
    "            self.history['epoch'] = hist.epoch\n",
    "\n",
    "    def show_training_errors(self):\n",
    "        hist = self.history\n",
    "\n",
    "        fig1 = plt.figure()\n",
    "        fig1, ax = plt.subplots(nrows=2, ncols=2, figsize=(15,12))\n",
    "\n",
    "        ax[0][0].set_xlabel('Epoch')\n",
    "        ax[0][0].set_ylabel('MSE')\n",
    "        ax[0][0].plot(hist['epoch'], hist['MSE'], label='Train Error')\n",
    "        ax[0][0].plot(hist['epoch'], hist['val_MSE'], label='Val Error')\n",
    "        ax[0][0].legend()\n",
    "        ax[0][0].set_title(\"MSE Error\")\n",
    "\n",
    "        ax[0][1].set_xlabel('Epoch')\n",
    "        ax[0][1].set_ylabel('MAE')\n",
    "        ax[0][1].plot(hist['epoch'], hist['MAE'], label='Train Error')\n",
    "        ax[0][1].plot(hist['epoch'], hist['val_MAE'], label='Val Error')\n",
    "        ax[0][1].legend()\n",
    "        ax[0][1].set_title(\"MAE Error\")\n",
    "\n",
    "        ax[1][0].set_xlabel('Epoch')\n",
    "        ax[1][0].set_ylabel('MAPE')\n",
    "        ax[1][0].plot(hist['epoch'], hist['MAPE'], label='Mean Abs {} Error'.format(\"%\"))\n",
    "        ax[1][0].legend()\n",
    "        ax[1][0].set_title(\"MAPE\")\n",
    "\n",
    "    def test(self, test_features, test_labels):\n",
    "        test_input = test_features\n",
    "        test_output = np.reshape(test_labels, (-1,1))\n",
    "        prediction = self.model.predict(test_input)\n",
    "\n",
    "        fig2 = plt.figure()\n",
    "        plt.plot(np.arange(len(test_output)), test_output, label=\"TestData\")\n",
    "        plt.plot(np.arange(len(prediction)), prediction, label=\"Prediction\", alpha=0.4)\n",
    "        plt.legend()\n",
    "        plt.title(\"Model Prediction\")"
   ]
  },
  {
   "source": [
    "Organise the data for training. Notably, the eta c values > 1.2 correspond to frac values => 0.8 with the largest (3) b1 values and the 0.025 to 0.15 a2 values. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_dataset.to_numpy()[:,0:4]\n",
    "train_labels = train_dataset.to_numpy()[:,4]\n",
    "\n",
    "# for noisey TRAINING data use this instead \n",
    "# train_features, train_labels = add_noise(train_dataset, noise_var=0.001, output_n=True)\n",
    "\n",
    "sorted_test = test_dataset.sort_index()\n",
    "test_features = sorted_test.to_numpy()[:,0:4]\n",
    "test_labels = sorted_test.to_numpy()[:,4]\n",
    "\n",
    "X = train_features.copy()\n",
    "Y = train_labels.copy()"
   ]
  },
  {
   "source": [
    "Start tests here."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# define input and output dimensions\n",
    "out_nodes = 1\n",
    "in_features = 4 \n",
    "\n",
    "summary = pd.DataFrame()\n",
    "final_metrics = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    # setup algo\n",
    "    my_model = MLP_Regression(in_features, nodes=30, dropout=False, rate=0.1)\n",
    "    my_model.learning_rate(initial_lr=1e-2, decay_steps=1e5, decay_rate=0.9)\n",
    "    my_model.optimizer(momentum=0.1)\n",
    "\n",
    "    # add layers\n",
    "    my_model.add_layer(23)\n",
    "    my_model.add_output_layer(out_nodes)\n",
    "    # my_model.add_dropout(0.1, seed=seed_val)\n",
    "\n",
    "    # run algo\n",
    "    my_model.compileModel(my_model.optimizer)\n",
    "    my_model.train_model(X,Y, epochs=200)\n",
    "\n",
    "    final_metrics.append(my_model.history.iloc[[-1]])\n",
    "\n",
    "summary = pd.concat(final_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.to_csv(\"/media/nirav/34E0-F309/KTH/Thesis/ModelSummaries/DropOut_01.csv\")"
   ]
  },
  {
   "source": [
    "4-2-1 was bad. \n",
    "Dropout=0.1, no epochs reached 50."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig =  plt.figure()\n",
    "\n",
    "# original\n",
    "load_data = pd.read_csv(\"/media/nirav/34E0-F309/KTH/Thesis/ModelSummaries/Reg5.csv\")\n",
    "x = load_data[load_data[\"epoch\"] > 50][\"val_MAPE\"].to_numpy()\n",
    "plt.plot(np.arange(0,len(x)), x, label='Original', marker='o')\n",
    "\n",
    "# Test 1, Changed 1h nodes\n",
    "load_data = pd.read_csv(\"/media/nirav/34E0-F309/KTH/Thesis/ModelSummaries/Reg5_33.csv\")\n",
    "x = load_data[load_data[\"epoch\"] > 50][\"val_MAPE\"].to_numpy()\n",
    "plt.plot(np.arange(0,len(x)), x, label='33-23-1', marker='o')\n",
    "\n",
    "# Test 2, Changed 1h nodes\n",
    "load_data = pd.read_csv(\"/media/nirav/34E0-F309/KTH/Thesis/ModelSummaries/Reg5_27.csv\")\n",
    "x = load_data[load_data[\"epoch\"] > 50][\"val_MAPE\"].to_numpy()\n",
    "plt.plot(np.arange(0,len(x)), x, label='27-23-1', marker='o')\n",
    "\n",
    "# Test 3, Changed momentum 0.1=>0.2\n",
    "load_data = pd.read_csv(\"/media/nirav/34E0-F309/KTH/Thesis/ModelSummaries/Mom_02.csv\")\n",
    "x = load_data[load_data[\"epoch\"] > 50][\"val_MAPE\"].to_numpy()\n",
    "plt.plot(np.arange(0,len(x)), x, label='Original, momentum=0.2', marker='o')\n",
    "\n",
    "# Test 4, Dropout on both, rate=0.1\n",
    "# load_data = pd.read_csv(\"/media/nirav/34E0-F309/KTH/Thesis/ModelSummaries/DropOut_01.csv\")\n",
    "# x = load_data[load_data[\"epoch\"] > 50][\"val_MAPE\"].to_numpy()\n",
    "# plt.plot(np.arange(0,len(x)), x, label='Original, Dropout=0.1', marker='o')\n",
    "\n",
    "# plt\n",
    "plt.legend()\n",
    "plt.ylabel(\"MAPE\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.title(\"MAPE over 10 sessions where # of Epochs was > 50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.concat(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}