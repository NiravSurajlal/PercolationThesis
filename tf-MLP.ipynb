{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "937a9bf7f4d02271b0e84c824f2cfec9d499b9b3c4d4ea54ebae47f399b1cd48"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Tensorflow MLP\n",
    "Here we will attempt to design a regression model for the data. First we will get the data from the new file then select which to use. The feature vector will be {b1, a2, frac, RatioTotalArea} and the target will be {eta c}. \n",
    "\n",
    "There are currently 4 options:\n",
    "- entire unscaled dataset: 57841\n",
    "- entire scaled dataset (from unscaled): 57841\n",
    "- limited dataset: 42016\n",
    "- limited and scaled dataset: 42016\n",
    "\n",
    "I will use the limited and scaled dataset to start because I can randomise easily. The data is scaled using the MaxAbsScaler to start.\n",
    "\n",
    "### I do think that we may be loosing data on the 2nd ellipse, so another feature may be necessary. (The info for the other dimension is there, but in the 'RatioTotalArea')."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Model Thoughts\n",
    "- Regression\n",
    "- Output activation = Linear, Hidden Layers= ReLu because +ve inputs and outputs\n",
    "- Loss: 'mean_squared_error' / 'mean_squared_logarithmic_error' = does not pinalise large values as much / 'mean_absolute_error' - more robust to outliers\n",
    "- Regularizers: Penalization in cost function to prevent overfitting. There is l1 and l2 + more. \n",
    "- Optimizers:\n",
    "- Dropout: Prevent overfitting. Maybe use rate of 0.2?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "seed_val = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_data(dataset, seed, train_ratio=0.6, test_ratio=0.2, shuffle=True):\n",
    "#     if shuffle:\n",
    "#         dataset = dataset.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "#     test_train_ratio = test_ratio/(1-train_ratio)\n",
    "\n",
    "#     train_dataset = dataset.sample(frac=train_ratio, random_state=0)\n",
    "#     valid_and_test_dataset = dataset.drop(train_dataset.index)\n",
    "\n",
    "#     test_dataset = valid_and_test_dataset.sample(frac=test_train_ratio, random_state=0)\n",
    "#     validation_dataset = valid_and_test_dataset.drop(test_dataset.index)\n",
    "    \n",
    "#     return train_dataset, test_dataset, validation_dataset\n",
    "\n",
    "def split_data(dataset, seed, train_ratio=0.6, shuffle=True):\n",
    "    if shuffle:\n",
    "        dataset = dataset.sample(frac=1, random_state=seed)#.reset_index(drop=True)\n",
    "\n",
    "    train_dataset = dataset.sample(frac=train_ratio, random_state=0)\n",
    "    test_dataset = dataset.drop(train_dataset.index)\n",
    "    \n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def add_bias(data):\n",
    "    N1 = np.shape(data)[0]\n",
    "    N2 = np.shape(data)[1]\n",
    "    a = -1*np.ones((N1,N2+1))\n",
    "    a[:,:-1] = data\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"LIM_scaled.csv\"\n",
    "name = \"data.csv\"\n",
    "\n",
    "dataset = pd.read_csv(name)\n",
    "# data2.describe().transpose()\n",
    "dataset.pop(\"Unnamed: 0\")\n",
    "dataset.describe().transpose()"
   ]
  },
  {
   "source": [
    "Scale the input features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_dataset = dataset.copy()\n",
    "\n",
    "scaled_dataset['b1'] = MaxAbsScaler().fit_transform(dataset['b1'].values.reshape(-1,1))\n",
    "scaled_dataset['a2'] = MaxAbsScaler().fit_transform(dataset['a2'].values.reshape(-1,1))\n",
    "scaled_dataset['RatioTotalArea'] = MaxAbsScaler().fit_transform(dataset['RatioTotalArea'].values.reshape(-1,1))\n",
    "scaled_dataset['frac'] = MaxAbsScaler().fit_transform(dataset['frac'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_dataset.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = split_data(scaled_dataset.copy(), seed_val, train_ratio=0.7)\n",
    "\n",
    "sorted_train = train_dataset.sort_index()\n",
    "sorted_test = test_dataset.sort_index()"
   ]
  },
  {
   "source": [
    "Check if the training and test data represent the data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(15,7))\n",
    "\n",
    "ax[0].scatter(x=np.arange(len(sorted_train)), y=sorted_train['eta c'], marker='.', alpha=0.4)\n",
    "ax[1].scatter(x=np.arange(len(sorted_test)), y=sorted_test['eta c'], marker='.', alpha=0.4)\n",
    "ax[0].set_ylabel(\"eta c\")\n",
    "ax[0].set_title(\"Training Data\")\n",
    "ax[1].set_title(\"Testing Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_dataset.to_numpy()[:,0:4]\n",
    "train_labels = train_dataset.to_numpy()[:,4]\n",
    "\n",
    "sorted_test = test_dataset.sort_index()\n",
    "test_features = sorted_test.to_numpy()[:,0:4]\n",
    "test_labels = sorted_test.to_numpy()[:,4]"
   ]
  },
  {
   "source": [
    "## Code the Model\n",
    "We have now scaled, shuffled and split the data. Have also checked that both the training and test set represent the output space. We can now move onto coding a model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_dataset.to_numpy()[:,0:4]\n",
    "train_labels = train_dataset.to_numpy()[:,4]\n",
    "\n",
    "out_nodes = 1\n",
    "in_features = 4 \n",
    "\n",
    "X = train_features.copy()\n",
    "Y = train_labels.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# early stopping\n",
    "earlystop_callback = EarlyStopping(monitor=\"loss\", min_delta=0, patience=3, mode=\"min\", restore_best_weights=True)\n",
    "\n",
    "# define the mlp model\n",
    "model = Sequential()\n",
    "\n",
    "# add the hidden layers and non-linear activation functions\n",
    "model.add(Dense(20, input_shape=(in_features,), activation=\"relu\", use_bias=True))\n",
    "keras.layers.Dropout(rate=0.2)\n",
    "model.add(Dense(15, activation=\"relu\", use_bias=True))\n",
    "keras.layers.Dropout(rate=0.2)\n",
    "# add the output layer\n",
    "model.add(Dense(out_nodes, activation=\"linear\", use_bias=True))\n",
    "\n",
    "# optimizer = keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "model.compile(loss=\"mean_squared_error\", optimizer='SGD', metrics=[\"MSE\",\"MAE\", \"accuracy\"])\n",
    "\n",
    "history = model.fit(X, Y, epochs=300, batch_size=500, validation_split=0.2, callbacks=[earlystop_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.plot(hist['epoch'], hist['MSE'], label='Train Error')\n",
    "plt.plot(hist['epoch'], hist['val_MSE'], label='Val Error')\n",
    "plt.legend()\n",
    "plt.title(\"Regression Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = test_features\n",
    "test_output = test_labels\n",
    "\n",
    "prediction = model.predict(test_input)\n",
    "accuracy = prediction-test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plt.figure()\n",
    "plt.plot(np.arange(len(test_output)), test_output, label=\"TestData\")\n",
    "plt.plot(np.arange(len(prediction)), prediction, label=\"Prediction\", alpha=0.4)\n",
    "# plt.plot(np.arange(len()),accuracy, label=\"Accuracy\", alpha=0.4)\n",
    "plt.legend()\n",
    "plt.title(\"Model Prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}