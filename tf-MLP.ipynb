{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "937a9bf7f4d02271b0e84c824f2cfec9d499b9b3c4d4ea54ebae47f399b1cd48"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Tensorflow MLP\n",
    "Here we will attempt to design a regression model for the data. First we will get the data from the new file then select which to use. The feature vector will be {b1, a2, frac, RatioTotalArea} and the target will be {eta c}. \n",
    "\n",
    "There are currently 4 options:\n",
    "- entire unscaled dataset: 57841\n",
    "- entire scaled dataset (from unscaled): 57841\n",
    "- limited dataset: 42016\n",
    "- limited and scaled dataset: 42016\n",
    "\n",
    "I will use the limited and scaled dataset to start because I can randomise easily. The data is scaled using the MaxAbsScaler to start.\n",
    "\n",
    "### I do think that we may be loosing data on the 2nd ellipse, so another feature may be necessary. (The info for the other dimension is there, but in the 'RatioTotalArea')."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Model Thoughts\n",
    "- Regression\n",
    "- Output activation = Linear, Hidden Layers= ReLu because +ve inputs and outputs\n",
    "- Loss: 'mean_squared_error' / 'mean_squared_logarithmic_error' = does not pinalise large values as much / 'mean_absolute_error' - more robust to outliers\n",
    "- Regularizers: Penalization in cost function to prevent overfitting. There is l1 and l2 + more. \n",
    "- Optimizers:\n",
    "- Dropout: Prevent overfitting. Maybe use rate of 0.2?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "seed_val = 97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_data(dataset, seed, train_ratio=0.6, test_ratio=0.2, shuffle=True):\n",
    "#     if shuffle:\n",
    "#         dataset = dataset.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "#     test_train_ratio = test_ratio/(1-train_ratio)\n",
    "\n",
    "#     train_dataset = dataset.sample(frac=train_ratio, random_state=0)\n",
    "#     valid_and_test_dataset = dataset.drop(train_dataset.index)\n",
    "\n",
    "#     test_dataset = valid_and_test_dataset.sample(frac=test_train_ratio, random_state=0)\n",
    "#     validation_dataset = valid_and_test_dataset.drop(test_dataset.index)\n",
    "    \n",
    "#     return train_dataset, test_dataset, validation_dataset\n",
    "\n",
    "def split_data(dataset, seed, train_ratio=0.6, shuffle=True):\n",
    "    if shuffle:\n",
    "        dataset = dataset.sample(frac=1, random_state=seed)#.reset_index(drop=True)\n",
    "\n",
    "    train_dataset = dataset.sample(frac=train_ratio, random_state=0)\n",
    "    test_dataset = dataset.drop(train_dataset.index)\n",
    "    \n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def add_bias(data):\n",
    "    N1 = np.shape(data)[0]\n",
    "    N2 = np.shape(data)[1]\n",
    "    a = -1*np.ones((N1,N2+1))\n",
    "    a[:,:-1] = data\n",
    "    return a\n",
    "\n",
    "def add_noise(dataset, target_column=4, noise_var=0.01, input_n=False, output_n=False):\n",
    "    \"\"\" Called on DATAFRAME training data. \"\"\"\n",
    "    features = dataset.to_numpy()[:,0:target_column]\n",
    "    labels = np.reshape(dataset.to_numpy()[:,target_column], (-1,1))\n",
    "    \n",
    "    if input_n:\n",
    "        noise = np.reshape(np.random.normal(0,0.01,np.shape(features)[0]*np.shape(features)[1]),(np.shape(features)[0],np.shape(features)[1]))\n",
    "        features = features + noise \n",
    "\n",
    "    if output_n:\n",
    "        noise = np.reshape(np.random.normal(0,0.01,np.shape(labels)[0]*np.shape(labels)[1]),(np.shape(labels)[0],np.shape(labels)[1]))\n",
    "        labels = labels + noise\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"LIM_scaled.csv\"\n",
    "name = \"data.csv\"\n",
    "\n",
    "dataset = pd.read_csv(name)\n",
    "# data2.describe().transpose()\n",
    "dataset.pop(\"Unnamed: 0\")\n",
    "dataset.describe().transpose()"
   ]
  },
  {
   "source": [
    "Scale the input features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_dataset = dataset.copy()\n",
    "\n",
    "scaled_dataset['b1'] = MaxAbsScaler().fit_transform(dataset['b1'].values.reshape(-1,1))\n",
    "scaled_dataset['a2'] = MaxAbsScaler().fit_transform(dataset['a2'].values.reshape(-1,1))\n",
    "scaled_dataset['RatioTotalArea'] = MaxAbsScaler().fit_transform(dataset['RatioTotalArea'].values.reshape(-1,1))\n",
    "scaled_dataset['frac'] = MaxAbsScaler().fit_transform(dataset['frac'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_dataset.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = split_data(scaled_dataset.copy(), seed_val, train_ratio=0.7)\n",
    "\n",
    "sorted_train = train_dataset.sort_index()\n",
    "sorted_test = test_dataset.sort_index()"
   ]
  },
  {
   "source": [
    "Check if the training and test data represent the data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(15,7))\n",
    "\n",
    "ax[0].scatter(x=np.arange(len(sorted_train)), y=sorted_train['eta c'], marker='.', alpha=0.4)\n",
    "ax[1].scatter(x=np.arange(len(sorted_test)), y=sorted_test['eta c'], marker='.', alpha=0.4)\n",
    "ax[0].set_ylabel(\"eta c\")\n",
    "ax[0].set_title(\"Training Data\")\n",
    "ax[1].set_title(\"Testing Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_dataset.to_numpy()[:,0:4]\n",
    "train_labels = train_dataset.to_numpy()[:,4]\n",
    "\n",
    "# for noisey TRAINING data use this instead \n",
    "# train_features, train_labels = add_noise(train_dataset, input_n=True, output_n=True)\n",
    "\n",
    "sorted_test = test_dataset.sort_index()\n",
    "test_features = sorted_test.to_numpy()[:,0:4]\n",
    "test_labels = sorted_test.to_numpy()[:,4]"
   ]
  },
  {
   "source": [
    "## Code the Model\n",
    "We have now scaled, shuffled and split the data. Have also checked that both the training and test set represent the output space. We can now move onto coding a model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_number = 6\n",
    "\n",
    "train_features = train_dataset.to_numpy()[:,0:4]\n",
    "train_labels = train_dataset.to_numpy()[:,4]\n",
    "\n",
    "# for noisey TRAINING data use this instead \n",
    "# train_features, train_labels = add_noise(train_dataset, noise_var=0.001, output_n=True)\n",
    "\n",
    "out_nodes = 1\n",
    "in_features = 4 \n",
    "\n",
    "X = train_features.copy()\n",
    "Y = train_labels.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping\n",
    "earlystop_callback = EarlyStopping(monitor=\"loss\", min_delta=0, patience=3, mode=\"min\", restore_best_weights=True)\n",
    "\n",
    "# setupt learning rate decay\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-2,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)\n",
    "    \n",
    "# setup the optimizer\n",
    "optimizer = keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.1, nesterov=False)\n",
    "\n",
    "# kernel initializers for the wieghts of each layer\n",
    "init_h1_kern = keras.initializers.RandomNormal(mean=0.0, stddev=0.54, seed=seed_val)\n",
    "init_h2_kern = keras.initializers.RandomNormal(mean=0.0, stddev=0.54, seed=(seed_val+1))\n",
    "init_out_kern = keras.initializers.RandomNormal(mean=0.0, stddev=0.54, seed=(seed_val+2))\n",
    "init_bias = keras.initializers.RandomNormal(mean=0.0, stddev=0.54, seed=(seed_val+3))\n",
    "# init_h1_kern = keras.initializers.Ones()\n",
    "# init_h2_kern = keras.initializers.Ones()\n",
    "# init_out_kern = keras.initializers.Ones()\n",
    "# init_bias = keras.initializers.Zeros()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the mlp model\n",
    "model = Sequential()\n",
    "\n",
    "# add the hidden layers and non-linear activation functions\n",
    "model.add(Dense(30, input_shape=(in_features,), activation=\"relu\", use_bias=True, kernel_initializer=init_h1_kern, bias_initializer=init_bias))\n",
    "# model.add(keras.layers.Dropout(rate=0.2, seed=seed_val*10))\n",
    "\n",
    "model.add(Dense(23, activation=\"relu\", use_bias=True, kernel_initializer=init_h2_kern, bias_initializer=init_bias))\n",
    "# keras.layers.Dropout(rate=0.2, seed=seed_val*20)\n",
    "\n",
    "# add the output layer\n",
    "# output_activation = keras.activations.relu(alpha=0.0, max_value=1.3, threshold=0)\n",
    "model.add(Dense(out_nodes, activation=\"relu\", use_bias=True, kernel_initializer=init_out_kern, bias_initializer=init_bias))\n",
    "\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=optimizer, metrics=[\"MSE\",\"MAE\", \"MAPE\"])\n",
    "\n",
    "history = model.fit(X, Y, epochs=10, batch_size=500, validation_split=0.2, callbacks=[earlystop_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "\n",
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "fig1, ax = plt.subplots(nrows=2, ncols=2, figsize=(15,12))\n",
    "\n",
    "ax[0][0].set_xlabel('Epoch')\n",
    "ax[0][0].set_ylabel('MSE')\n",
    "ax[0][0].plot(hist['epoch'], hist['MSE'], label='Train Error')\n",
    "ax[0][0].plot(hist['epoch'], hist['val_MSE'], label='Val Error')\n",
    "ax[0][0].legend()\n",
    "ax[0][0].set_title(\"MSE Error\")\n",
    "\n",
    "ax[0][1].set_xlabel('Epoch')\n",
    "ax[0][1].set_ylabel('MAE')\n",
    "ax[0][1].plot(hist['epoch'], hist['MAE'], label='Train Error')\n",
    "ax[0][1].plot(hist['epoch'], hist['val_MAE'], label='Val Error')\n",
    "ax[0][1].legend()\n",
    "ax[0][1].set_title(\"MAE Error\")\n",
    "\n",
    "ax[1][0].set_xlabel('Epoch')\n",
    "ax[1][0].set_ylabel('MAPE')\n",
    "ax[1][0].plot(hist['epoch'], hist['MAPE'], label='Mean Abs {} Error'.format(\"%\"))\n",
    "ax[1][0].legend()\n",
    "ax[1][0].set_title(\"MAPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = test_features\n",
    "test_output = np.reshape(test_labels, (-1,1))\n",
    "\n",
    "prediction = model.predict(test_input)\n",
    "accuracy = prediction-test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plt.figure()\n",
    "plt.plot(np.arange(len(test_output)), test_output, label=\"TestData\")\n",
    "plt.plot(np.arange(len(prediction)), prediction, label=\"Prediction\", alpha=0.4)\n",
    "# plt.plot(np.arange(len(accuracy)),accuracy, label=\"Accuracy\", alpha=0.4)\n",
    "plt.legend()\n",
    "plt.title(\"Model Prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3 = plt.figure()\n",
    "plt.plot(np.arange(len(accuracy)),accuracy, label=\"Accuracy\", alpha=0.4)\n",
    "plt.legend()\n",
    "plt.title(\"Model Accuracy\")"
   ]
  },
  {
   "source": [
    "# CHANGE MODEL NUMBER"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_number = 6\n",
    "model.save(\"NewModelTests/Regression{}\".format(model_number))"
   ]
  },
  {
   "source": [
    "## Test Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = test_features\n",
    "test_output = test_labels\n",
    "\n",
    "prediction = model.predict(test_input)\n",
    "accuracy = prediction-test_output\n",
    "\n",
    "\n",
    "fig2 = plt.figure()\n",
    "plt.plot(np.arange(len(test_output)), test_output, label=\"TestData\")\n",
    "plt.plot(np.arange(len(prediction)), prediction, label=\"Prediction\", alpha=0.4)\n",
    "# plt.plot(np.arange(len(accuracy)),accuracy, label=\"Accuracy\", alpha=0.4)\n",
    "plt.legend()\n",
    "plt.title(\"Model Prediction\")"
   ]
  },
  {
   "source": [
    "Using a new subset of the data. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "name = \"data.csv\" \n",
    "dataset = pd.read_csv(name)\n",
    "dataset.pop(\"Unnamed: 0\")\n",
    "\n",
    "# Scale the data \n",
    "scaled_dataset = dataset.copy()\n",
    "scaled_dataset['b1'] = MaxAbsScaler().fit_transform(dataset['b1'].values.reshape(-1,1))\n",
    "scaled_dataset['a2'] = MaxAbsScaler().fit_transform(dataset['a2'].values.reshape(-1,1))\n",
    "scaled_dataset['RatioTotalArea'] = MaxAbsScaler().fit_transform(dataset['RatioTotalArea'].values.reshape(-1,1))\n",
    "scaled_dataset['frac'] = MaxAbsScaler().fit_transform(dataset['frac'].values.reshape(-1,1))\n",
    "\n",
    "# Split the data \n",
    "# new seed value is required !=2000\n",
    "train_dataset, test_dataset = split_data(scaled_dataset.copy(), 99, train_ratio=0.7)\n",
    " \n",
    "# Prepare for model\n",
    "# Only use 'test_features' and 'test_labels' at this stage \n",
    "train_features = train_dataset.to_numpy()[:,0:4]\n",
    "train_labels = train_dataset.to_numpy()[:,4]\n",
    " \n",
    "sorted_test = test_dataset.sort_index()\n",
    "test_features = sorted_test.to_numpy()[:,0:4]\n",
    "test_labels = sorted_test.to_numpy()[:,4]\n",
    "\n",
    "test_input = test_features\n",
    "test_output = np.reshape(test_labels, (-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_choice1 = 1\n",
    "model1 = keras.models.load_model(\"Model/Regression{}\".format(model_choice1))\n",
    "model_choice2 = 2\n",
    "model2 = keras.models.load_model(\"Model/Regression{}\".format(model_choice2))\n",
    "\n",
    "#_________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
    "prediction1 = model1.predict(test_input)\n",
    "accuracy1 = prediction1-test_output\n",
    "\n",
    "prediction2 = model2.predict(test_input)\n",
    "accuracy2 = prediction2-test_output\n",
    "\n",
    "#_________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
    "fig1 = plt.figure()\n",
    "fig1, ax = plt.subplots(ncols=2,figsize=(18,5))\n",
    "\n",
    "ax[0].plot(np.arange(len(test_output)), test_output, label=\"TestData\")\n",
    "ax[0].plot(np.arange(len(prediction1)), prediction1, label=\"Prediction\", alpha=0.4)\n",
    "ax[0].set_title(\"Model{}\".format(model_choice1))\n",
    "ax[0].set_ylabel(\"Eta c\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(np.arange(len(test_output)), test_output, label=\"TestData\")\n",
    "ax[1].plot(np.arange(len(prediction2)), prediction2, label=\"Prediction\", alpha=0.4)\n",
    "ax[1].set_title(\"Model{}\".format(model_choice2))\n",
    "ax[1].set_ylabel(\"Eta c\")\n",
    "ax[1].legend()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_choice1 = 3\n",
    "model1 = keras.models.load_model(\"Model/Regression{}\".format(model_choice1))\n",
    "model_choice2 = 4\n",
    "model2 = keras.models.load_model(\"Model/Regression{}\".format(model_choice2))\n",
    "\n",
    "#_________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
    "prediction1 = model1.predict(test_input)\n",
    "accuracy1 = prediction1-test_output\n",
    "\n",
    "prediction2 = model2.predict(test_input)\n",
    "accuracy2 = prediction2-test_output\n",
    "\n",
    "#_________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
    "fig2 = plt.figure()\n",
    "fig2, ax = plt.subplots(ncols=2,figsize=(18,5))\n",
    "\n",
    "ax[0].plot(np.arange(len(test_output)), test_output, label=\"TestData\")\n",
    "ax[0].plot(np.arange(len(prediction1)), prediction1, label=\"Prediction\", alpha=0.4)\n",
    "ax[0].set_title(\"Model{}\".format(model_choice1))\n",
    "ax[0].set_ylabel(\"Eta c\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(np.arange(len(test_output)), test_output, label=\"TestData\")\n",
    "ax[1].plot(np.arange(len(prediction2)), prediction2, label=\"Prediction\", alpha=0.4)\n",
    "ax[1].set_title(\"Model{}\".format(model_choice2))\n",
    "ax[1].set_ylabel(\"Eta c\")\n",
    "ax[1].legend()\n",
    "\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_choice1 = 5\n",
    "model1 = keras.models.load_model(\"Model/Regression{}\".format(model_choice1))\n",
    "model_choice2 = 6\n",
    "model2 = keras.models.load_model(\"Model/Regression{}\".format(model_choice2))\n",
    "\n",
    "#_________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
    "prediction1 = model1.predict(test_input)\n",
    "accuracy1 = prediction1-test_output\n",
    "\n",
    "prediction2 = model2.predict(test_input)\n",
    "accuracy2 = prediction2-test_output\n",
    "\n",
    "#_________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
    "fig3 = plt.figure()\n",
    "fig3, ax = plt.subplots(ncols=2,figsize=(18,5))\n",
    "\n",
    "ax[0].plot(np.arange(len(test_output)), test_output, label=\"TestData\")\n",
    "ax[0].plot(np.arange(len(prediction1)), prediction1, label=\"Prediction\", alpha=0.4)\n",
    "ax[0].set_title(\"Model{}\".format(model_choice1))\n",
    "ax[0].set_ylabel(\"Eta c\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(np.arange(len(test_output)), test_output, label=\"TestData\")\n",
    "ax[1].plot(np.arange(len(prediction2)), prediction2, label=\"Prediction\", alpha=0.4)\n",
    "ax[1].set_title(\"Model{}\".format(model_choice2))\n",
    "ax[1].set_ylabel(\"Eta c\")\n",
    "ax[1].legend()\n",
    "\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3"
   ]
  },
  {
   "source": [
    "## Change output layer to Relu"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model = keras.models.load_model(\"Model/Regression5\")\n",
    "original_prediction = original_model.predict(test_input)\n",
    "\n",
    "new_model = keras.models.load_model(\"Model/Regression5\")\n",
    "new_model.layers.pop(2) \n",
    "new_model.add(Dense(out_nodes, activation=\"relu\", use_bias=True))\n",
    "new_prediction = new_model.predict(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig, ax = plt.subplots(ncols=2,figsize=(18,5))\n",
    "\n",
    "ax[0].plot(np.arange(len(test_output)), test_output, label=\"TestData\")\n",
    "ax[0].plot(np.arange(len(original_prediction)), original_prediction, label=\"Prediction\", alpha=0.4)\n",
    "ax[0].set_title(\"Original Model\")\n",
    "ax[0].set_ylabel(\"Eta c\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(np.arange(len(test_output)), test_output, label=\"TestData\")\n",
    "ax[1].plot(np.arange(len(new_prediction)), new_prediction, label=\"Prediction\", alpha=0.4)\n",
    "ax[1].set_title(\"New Model\")\n",
    "ax[1].set_ylabel(\"Eta c\")\n",
    "ax[1].legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig, ax = plt.subplots(ncols=2,figsize=(18,5))\n",
    "\n",
    "ax[0].plot(np.arange(len(accuracy1)), accuracy1, label=\"Prediction\", alpha=0.4)\n",
    "ax[0].set_title(\"Model1\")\n",
    "ax[0].set_ylabel(\"Accuracy\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(np.arange(len(accuracy2)), accuracy2, label=\"Prediction\", alpha=0.4)\n",
    "ax[1].set_title(\"Model2\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(accuracy1)), accuracy1, label=\"Acc1\")\n",
    "plt.plot(np.arange(len(accuracy2)), accuracy2, label=\"Acc2\", alpha=0.4)\n",
    "plt.title(\"Models 1 & 4\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}